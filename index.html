<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Ishan Dave</title>
  
  <meta name="author" content="Ishan Dave">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<!-- <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>"> -->
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Ishan Dave</name>
              </p>
              <p style="font-size:15px">I am a fifth-year Ph.D. student in the Center for Research in Computer Vision (CRCV), University of Central Florida (UCF), advised by <a href="https://www.crcv.ucf.edu/person/mubarak-shah/" style="font-size:15px">Prof. Mubarak Shah</a>. </a>.
              </p>
              <p style="text-align:center">
                <a href="mailto:ishandave95@gmail.com">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?hl=en&user=fWu6sFgAAAAJ&view_op=list_works&sortby=pubdate">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/daveishan">Github</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/ishan-dave-crcv/">LinkedIn</a> &nbsp/&nbsp
                <a href="data/CV_Ishan_public_14Dec2023.pdf">CV</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/ishan_pic-modified.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/ishan_pic-modified.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Work Experience</heading>
            </td>
          </tr>
          </table>
          <table width="100%" align="center" border="0" cellpadding="20"></tbody>

            <tr>
              <td style="padding:10px;width:25%;vertical-align:top">
		<img src='images/Adobe-logo.png' style="width:100%;">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <strong>Research Scientist/ Engineer Intern</strong>
                <br> Adobe Inc., San Jose, California, USA. May 2023- Nov 2023
                <br> Host: <a href="https://sjenni.github.io/">Simon Jenni, </a>
                <a href="https://fabiancaba.com/">Fabian Caba</a>
                <p></p>
                <p> Developed fine-grained video retrieval systems for large-scale video galleries with millions of videos</p>
              </td>
            </tr>
          </tr>
          </table>
	 <table width="100%" align="center" border="0" cellpadding="20"></tbody>

            <tr>
               <td style="padding:10px;width:25%;vertical-align:top">
		<img src='images/Adobe-logo.png' style="width:100%;">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <strong>Research Scientist Intern</strong>
                <br> Adobe Inc., Remote, USA. May 2022 - Nov 2022
                <br> Host: <a href="https://sjenni.github.io/">Simon Jenni</a>
                <p></p>
                <p> Proposed Self-supervised Video Representation Learning method suitable for both high-level and low-level video downstream tasks </p>
              </td>
            </tr>
          </tr>
          </table>

         

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                I have a broad interest in computer vision and machine learning. My current research mainly focuses on video representation learning with limited labels (self/semi-supervised learning), action recognition, and privacy preservation in video understanding tasks. I have also worked on various robotics-related vision tasks like event-camera-based action recognition and drone-to-drone detections from videos. 
                Below is a selected list of my works (in chronological order), representative papers are <span style="background-color: #ffffd0;">highlighted</span>.

              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

	<tr onmouseout="mira_stop()" onmouseover="mira_start()" bgcolor="#ffffd0">
	    <td style="padding:20px;width:25%;vertical-align:middle">
	        <div class="one">
	            <img src='images/nms_aaai2024.png' width="180">
	        </div>
	    </td>
	    <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="">
            <papertitle>No More Shortcuts: Realizing the Potential of Temporal Self-Supervision</papertitle>
        </a>
        <br>
        <strong>Ishan Rajendrakumar Dave</strong>, 
        Simon Jenni,  
        Mubarak Shah.
        <br>
        <em>AAAI Conference on Artificial Intelligence (<strong>AAAI</strong>)</em>, 2024
        <br>
        <a href="">paper(Coming Soon) </a> &nbsp/&nbsp <a href="">code (Coming Soon)</a>
   	<p></p>
	<p>
		We demonstrate experimentally that our more challenging frame-level task formulations and the removal of shortcuts drastically improve the quality of features learned through temporal self-supervision. Our extensive experiments show state-of-the-art performance across 10 video understanding datasets, illustrating the generalization ability and robustness of our learned video representations.
	</p>
    	</td>
	</tr>
		
		
	<tr onmouseout="mira_stop()" onmouseover="mira_start()">
	    <td style="padding:20px;width:25%;vertical-align:middle">
		<div class="one">
		    <img src='images/tedspad_iccv2023.png' width="180">
		</div>
	    </td>
	    <td style="padding:20px;width:75%;vertical-align:middle">
		<a href="LINK_TO_PAPER">
		    <papertitle>TeD-SPAD: Temporal Distinctiveness for Self-supervised Privacy-preservation for Video Anomaly Detection</papertitle>
		</a>
		<br>
		Joseph Fioresi, 
		<strong>Ishan Rajendrakumar Dave</strong>, 
		Mubarak Shah.
		<br>
		<em>Proceedings of the IEEE/CVF International Conference on Computer Vision (<strong>ICCV</strong>)</em>, 2023
		<br>
		<a href="https://arxiv.org/pdf/2308.11072.pdf">paper</a> &nbsp/&nbsp <a href="https://github.com/UCF-CRCV/TeD-SPAD">code</a>
		    <p></p>
			<p>
			    We propose TeD-SPAD, a privacy-aware video anomaly detection framework that destroys visual private information in a self-supervised manner. In particular, we propose the use of a temporally-distinct triplet loss to promote temporally discriminative features, which complements current weakly-supervised VAD methods.
			</p>
	    </td>
	</tr>

	<tr onmouseout="mira_stop()" onmouseover="mira_start()">
	    <td style="padding:20px;width:25%;vertical-align:middle">
	        <div class="one">
	            <img src='images/eventtransact_iros2023.png' width="180">
	        </div>
	    </td>
	    <td style="padding:20px;width:75%;vertical-align:middle">
	        <a href="LINK_TO_PAPER">
	            <papertitle>EventTransAct: A Video Transformer-based Framework for Event-camera Based Action Recognition</papertitle>
	        </a>
	        <br>
	        Tristan de Blegiers*, 
	        <strong>Ishan Rajendrakumar Dave*</strong>, 
	        Adeel Yousaf, 
	        Mubarak Shah.
		<br>
		*= equal contribution
	        <br>
	        <em>IEEE/RSJ International Conference on Intelligent Robots and Systems (<strong>IROS</strong>)</em>, 2023
	        <br>
	        <a href="LINK_TO_PAPER">paper</a> &nbsp/&nbsp <a href="https://github.com/tristandb8/EventTransAct">code</a>
	    	<p></p>
		<p>
		   We propose a video transformer-based framework for event-camera based action recognition, which leverages event-contrastive loss and augmentations to adapt the network to event data. Our method achieved state-of-the-art results on N-EPIC Kitchens dataset and competitive results on the standard DVS Gesture recognition dataset, while requiring less computation time compared to competitive prior approaches. 
		</p>
	    </td>
	</tr>

		
		
          <tr onmouseout="mira_stop()" onmouseover="mira_start()" bgcolor="#ffffd0">
	    <td style="padding:20px;width:25%;vertical-align:middle">
	        <div class="one">
	            <img src='images/timebalance_cvpr2023.png' width="180">
	        </div>
	    </td>
	    <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Dave_TimeBalance_Temporally-Invariant_and_Temporally-Distinctive_Video_Representations_for_Semi-Supervised_Action_Recognition_CVPR_2023_paper.pdf">
            <papertitle>TimeBalance: Temporally-Invariant and Temporally-Distinctive Video Representations for Semi-Supervised Action Recognition</papertitle>
        </a>
        <br>
        <strong>Ishan Rajendrakumar Dave</strong>, 
        Mamshad Nayeem Rizve, 
        Chen Chen, 
        Mubarak Shah.
        <br>
        <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2023
        <br>
        <a href="https://arxiv.org/pdf/2303.16268.pdf">paper</a> &nbsp/&nbsp <a href="https://github.com/DAVEISHAN/TimeBalance">code</a>
   	<p></p>
	<p>
		We propose a student-teacher semi-supervised learning framework, where we distill knowledge from a temporally-invariant and temporally-distinctive teacher. Depending on the nature of the unlabeled video, we dynamically combine the knowledge of these two teachers based on a novel temporal similarity-based reweighting scheme. State-of-the-art results on Kinetics400, UCF101, HMDB51.

	</p>
    	</td>
	</tr>

	<tr onmouseout="mira_stop()" onmouseover="mira_start()">
	    <td style="padding:20px;width:25%;vertical-align:middle">
	        <div class="one">
	            <img src='images/transvisdrone_icra23.png' width="180">
	        </div>
	    </td>
	    <td style="padding:20px;width:75%;vertical-align:middle">
	        <a href="https://ieeexplore.ieee.org/document/10161433">
	            <papertitle>Transvisdrone: Spatio-temporal Transformer for Vision-based Drone-to-drone Detection in Aerial Videos</papertitle>
	        </a>
	        <br>
	        Tushar Sangam, 
	        <strong>Ishan Rajendrakumar Dave</strong>,
	        Waqas Sultani, 
	        Mubarak Shah.
	        <br>
	        <em>2023 IEEE International Conference on Robotics and Automation (<strong>ICRA</strong>)</em>, 2023
	        <br>
	        <a href="https://arxiv.org/pdf/2210.08423.pdf">paper</a> &nbsp/&nbsp <a href="https://github.com/tusharsangam/TransVisDrone">code</a>
			<p></p>
			<p>
			    We propose a simple yet effective framework, TransVisDrone, that provides an end-to-end solution with higher computational efficiency. We utilize CSPDarkNet-53 network to learn object-related spatial features and VideoSwin model to improve drone detection in challenging scenarios by learning spatio-temporal dependencies of drone motion.
			</p>
	    </td>
	</tr>

	

	<tr onmouseout="mira_stop()" onmouseover="mira_start()" bgcolor="#ffffd0">
	    <td style="padding:20px;width:25%;vertical-align:middle">
	        <div class="one">
	            <img src='images/spact_cvpr2022.png' width="180">
	        </div>
	    </td>
	    <td style="padding:20px;width:75%;vertical-align:middle">
	        <a href="https://openaccess.thecvf.com/content/CVPR2022/html/Dave_SPAct_Self-Supervised_Privacy_Preservation_for_Action_Recognition_CVPR_2022_paper.html">
	            <papertitle>SPAct: Self-supervised Privacy Preservation for Action Recognition</papertitle>
	        </a>
	        <br>
	        <strong>Ishan Rajendrakumar Dave</strong>, 
	        Chen Chen, 
	        Mubarak Shah.
	        <br>
	        <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2022
	        <br>
	        <a href="https://arxiv.org/pdf/2203.15205.pdf">paper</a> &nbsp/&nbsp <a href="https://github.com/DAVEISHAN/SPAct">code</a>
		    <p></p>
			<p>
			    For the first time, we present a novel training framework that removes privacy information from input video in a self-supervised manner without requiring privacy labels. We train our framework using a minimax optimization strategy to minimize the action recognition cost function and maximize the privacy cost function through a contrastive self-supervised loss. 
			</p>
	    </td>
	</tr>

	

	
		
	<tr onmouseout="mira_stop()" onmouseover="mira_start()" bgcolor="#ffffd0">
	    <td style="padding:5px;width:25%;vertical-align:middle">
	        <div class="one">
	            <img src='images/tclr_cviu2022.png' width="180">
	        </div>
	        <script type="text/javascript">
	            function mira_start() {
	                document.getElementById('mira_image').style.opacity = "1";
	            }
	
	            function mira_stop() {
	                document.getElementById('mira_image').style.opacity = "0";
	            }
	            mira_stop()
	        </script>
	    </td>
	    <td style="padding:20px;width:75%;vertical-align:middle">
	        <a href="https://www.sciencedirect.com/science/article/pii/S1077314222000376">
	            <papertitle>TCLR: Temporal Contrastive Learning for Video Representation</papertitle>
	        </a>
	        <br>
	        <strong>Ishan Dave</strong>,
	        Rohit Gupta, 
	        Mamshad Nayeem Rizve, 
	        Mubarak Shah.
	        <br>
	        <em>Computer Vision and Image Understanding (<strong>CVIU</strong>)</em>, 2022
	        <br>
	       <em><strong><font color="red">(100+ citations, Among the top-10 most downloaded papers in CVIU)</font></strong></em>
               <br>
	        <a href="https://arxiv.org/pdf/2101.07974.pdf">paper</a> &nbsp/&nbsp
	        <a href="https://github.com/DAVEISHAN/TCLR">code</a>
	        <p></p>
	        <p>
	            We propose a new temporal contrastive learning framework for self-supervised video representation learning, consisting of two novel losses that aim to increase the temporal diversity of learned features. The framework achieves state-of-the-art results on various downstream video understanding tasks, including significant improvement in fine-grained action classification for visually similar classes.
	        </p>
	    </td>
	</tr>

	<tr onmouseout="mira_stop()" onmouseover="mira_start()">
	    <td style="padding:20px;width:25%;vertical-align:middle">
	        <div class="one">
	            <img src='images/gabriellav2_wacv22.png' width="180">
	        </div>
	    </td>
	    <td style="padding:20px;width:75%;vertical-align:middle">
	        <a href="https://openaccess.thecvf.com/content/WACV2022W/HADCV/papers/Dave_GabriellaV2_Towards_Better_Generalization_in_Surveillance_Videos_for_Action_Detection_WACVW_2022_paper.pdf">
	            <papertitle>Gabriellav2: Towards Better Generalization in Surveillance Videos for Action Detection</papertitle>
	        </a>
	        <br>
	        <strong>Ishan Dave</strong>, 
	        Zacchaeus Scheffer, 
	        Akash Kumar, 
	        Sarah Shiraz, 
	        Yogesh Singh Rawat, 
	        Mubarak Shah.
	        <br>
	        <em>Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (<strong>WACV</strong>)</em>, 2022
	        <br>
	        <a href="https://openaccess.thecvf.com/content/WACV2022W/HADCV/papers/Dave_GabriellaV2_Towards_Better_Generalization_in_Surveillance_Videos_for_Action_Detection_WACVW_2022_paper.pdf">paper</a>
	        <p></p>
	        <p>
	            We propose a realtime, online, action detection system which can generalize robustly on any unknown facility surveillance videos. We tackle the
challenging nature of action classification problem in various aspects like handling the class-imbalance training using PLM method and learning multi-label action correlations using LSEP loss. In order to improve the computational efficiency of the system, we utilize knowledge distillation.
	        </p>
	    </td>
	</tr>	
	<tr onmouseout="mira_stop()" onmouseover="mira_start()">
	    <td style="padding:20px;width:25%;vertical-align:middle">
	        <div class="one">
	            <img src='images/gabriella_icpr2020.png' width="180">
	        </div>
	    </td>
	    <td style="padding:20px;width:75%;vertical-align:middle">
	        <a href="https://ieeexplore.ieee.org/document/9412791">
	            <papertitle>Gabriella: An Online System for Real-Time Activity Detection in Untrimmed Security Videos</papertitle>
	        </a>
	        <br>
	        Mamshad Nayeem Rizve, 
	        Ugur Demir, 
	        Praveen Tirupattur, 
	        Aayush Jung Rana, 
	        Kevin Duarte, 
	        <strong>Ishan R Dave</strong>, 
	        Yogesh S Rawat, 
	        Mubarak Shah.
	        <br>
	        <em>25th International Conference on Pattern Recognition (<strong>ICPR</strong>)</em>, 2021 <em><strong><font color="red">(Best Paper Award)</font></strong></em>
	        <br>
	        <a href="https://arxiv.org/abs/2004.11475">paper</a> 
		<p></p>
	        <p>
	            Gabriella consists of three stages: tubelet extraction, activity classification, and online tubelet merging. Gabriella utilizes a localization network for tubelet extraction, with a novel Patch-Dice loss to handle variations in actor size, and a Tubelet-Merge Action-Split (TMAS) algorithm to detect activities efficiently and robustly.
	        </p>    
	    </td>
	</tr>

	<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Patent</heading>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

	<tr onmouseout="mira_stop()" onmouseover="mira_start()">
	    <td style="padding:20px;width:25%;vertical-align:middle">
		<div class="one">
		    <img src='images/spact_patent2023.jpg' width="180">
		</div>
	    </td>
	    <td style="padding:20px;width:75%;vertical-align:middle">
		<a href="LINK_TO_PAPER">
		    <papertitle>Action Recognition System Preserves Privacy in Video Sharing.</papertitle>
		</a>
		<br>
		<strong>Ishan Rajendrakumar Dave</strong>, 
		Chen Chen,
		Mubarak Shah, 	
		<br>
		<em> The University of Central Florida. Invention Track Code: 2023-019. (Status: Filed) </em>, 2023
		<br>
		<a href="https://ucf.flintbox.com/technologies/781d7a2d-1f89-41db-911c-5d4ecf26af20">Tech Sheet</a>
		    
	    </td>
	</tr>


		
				
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Professional Reviewing experience</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
					
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/cvf.jpg"></td>
            <td width="75%" valign="center">
		<a href="https://cvpr2022.thecvf.com/area-chairs">Reviewer, CVPR 2022, 2023</a><br>
		<a href="https://iccv2023.thecvf.com/">Reviewer, ICCV 2023</a>
 		<br>	
	      	<a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=83">Reviewer, IEEE Transaction on Image Processing</a><br>
	        <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34">Reviewer, IEEE Transaction on Pattern Analysis and Machine Intelligence</a>
              <br>
              <br>
             
		    
            </td>
          </tr>
	


		
	<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Mentor in NSF-REU</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
					
          <tr>
            <td style="padding:20px;width:25%;text-align:center;vertical-align:middle"><img src="images/NSF_svg.png" width="50%" alt="NSF Image"></td>
            <td width="75%" valign="center">
		<a href="https://www.crcv.ucf.edu/nsf-projects/reu/reu-2022/">Kevin Chung, REU 2022</a>
	        <br>

		<a href="https://www.crcv.ucf.edu/nsf-projects/reu/reu-2021/">Ethan Thomas, REU 2021</a>
		<br>
		<a href="https://www.crcv.ucf.edu/nsf-projects/reu/reu-2020/">Kali Carter, REU 2020</a>

              <br>
              <br>
		    
            </td>
          </tr>

		
					
					
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Feel free to steal this website's <a href="https://github.com/jonbarron/jonbarron_website">source code</a>. <strong>Do not</strong> scrape the HTML from this page itself, as it includes analytics tags that you do not want on your own website &mdash; use the github code instead. Also, consider using <a href="https://leonidk.com/">Leonid Keselman</a>'s <a href="https://github.com/leonidk/new_website">Jekyll fork</a> of this page.
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
