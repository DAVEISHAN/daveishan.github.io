<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Ishan Dave</title>
  
  <meta name="author" content="Ishan Dave">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.0.7/css/all.css">

	<!-- <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>"> -->
  
  <!-- Sticky Navigation Styles -->
  <style>
    .navbar {
      position: fixed;
      top: 0;
      left: 0;
      right: 0;
      background-color: rgba(255, 255, 255, 0.95);
      backdrop-filter: blur(10px);
      box-shadow: 0 2px 10px rgba(0, 0, 0, 0.1);
      z-index: 1000;
      padding: 12px 0;
    }
    
    .navbar-container {
      max-width: 800px;
      margin: 0 auto;
      display: flex;
      justify-content: center;
      align-items: center;
      flex-wrap: wrap;
      padding: 0 20px;
    }
    
    .navbar a {
      color: #333;
      text-decoration: none;
      padding: 8px 16px;
      margin: 0 4px;
      border-radius: 4px;
      font-size: 14px;
      font-weight: 500;
      transition: all 0.3s ease;
    }
    
    .navbar a:hover {
      background-color: #e8f4f8;
      color: #0066cc;
    }
    
    .navbar a.active {
      background-color: #0066cc;
      color: white;
    }
    
    html {
      scroll-behavior: smooth;
      scroll-padding-top: 80px;
    }
    
    body {
      padding-top: 60px;
    }
    
    @media (max-width: 768px) {
      .navbar a {
        padding: 6px 10px;
        font-size: 12px;
        margin: 2px;
      }
      body {
        padding-top: 80px;
      }
    }
    
    /* Collapsible description styles */
    .paper-description {
      max-height: 0;
      overflow: hidden;
      transition: max-height 0.3s ease-out;
    }
    
    .paper-description.expanded {
      max-height: 500px;
      transition: max-height 0.4s ease-in;
    }
    
    .toggle-description {
      color: #0066cc;
      cursor: pointer;
      font-size: 13px;
      text-decoration: none;
      display: inline-block;
      margin-top: 5px;
    }
    
    .toggle-description:hover {
      text-decoration: underline;
    }
    
    /* Paper link buttons */
    .paper-links {
      margin-top: 8px;
    }
    
    .paper-link-btn {
      display: inline-block;
      padding: 6px 14px;
      margin: 4px 6px 4px 0;
      border: 2px solid #4285f4;
      border-radius: 8px;
      color: #4285f4;
      text-decoration: none;
      font-size: 13px;
      font-weight: 600;
      text-transform: uppercase;
      transition: all 0.2s ease;
    }
    
    .paper-link-btn:hover {
      background-color: #4285f4;
      color: white;
    }
    
    /* Collapsible year dividers for updates */
    .year-divider {
      font-weight: 600;
      font-size: 16px;
      color: #333;
      margin-top: 18px;
      margin-bottom: 10px;
      padding-left: 0px;
      border-left: 3px solid #0066cc;
      padding-left: 10px;
      cursor: pointer;
      user-select: none;
    }
    
    .year-divider:hover {
      color: #0066cc;
    }
    
    .year-divider::before {
      content: '‚ñº ';
      display: inline-block;
      transition: transform 0.3s;
      font-size: 12px;
      margin-right: 5px;
    }
    
    .year-divider.collapsed::before {
      transform: rotate(-90deg);
    }
    
    .year-updates {
      max-height: 1000px;
      overflow: hidden;
      transition: max-height 0.4s ease-out, opacity 0.3s ease-out;
      opacity: 1;
    }
    
    .year-updates.collapsed {
      max-height: 0;
      opacity: 0;
      transition: max-height 0.3s ease-out, opacity 0.2s ease-out;
    }
  </style>
  
  <!-- Active Section Highlighting Script -->
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      const sections = document.querySelectorAll('[id]');
      const navLinks = document.querySelectorAll('.navbar a');
      
      function setActiveLink() {
        let currentSection = '';
        const scrollPosition = window.scrollY + 150; // Offset for navbar height
        
        // Convert sections to array and sort by position
        const sectionArray = Array.from(sections).filter(section => {
          // Only consider sections that have navigation links
          return section.id === 'about' || section.id === 'updates' || 
                 section.id === 'experience' || section.id === 'research' || 
                 section.id === 'patents' || section.id === 'recognition';
        }).sort((a, b) => a.offsetTop - b.offsetTop);
        
        // Find the current section
        for (let i = 0; i < sectionArray.length; i++) {
          const section = sectionArray[i];
          const sectionTop = section.offsetTop - 100; // Small offset
          const nextSection = sectionArray[i + 1];
          const sectionBottom = nextSection ? nextSection.offsetTop : document.body.scrollHeight;
          
          if (scrollPosition >= sectionTop && scrollPosition < sectionBottom) {
            currentSection = section.getAttribute('id');
            break;
          }
        }
        
        // If we're at the very top, default to first section
        if (!currentSection && scrollPosition < 200) {
          currentSection = 'about';
        }
        
        // Update active state
        navLinks.forEach(link => {
          link.classList.remove('active');
          if (link.getAttribute('href') === '#' + currentSection) {
            link.classList.add('active');
          }
        });
      }
      
      window.addEventListener('scroll', setActiveLink);
      setActiveLink(); // Set initial active state
    });
  </script>
  
  <!-- Description Toggle Script -->
  <script>
    function toggleDescription(id) {
      const desc = document.getElementById(id);
      const toggle = document.getElementById('toggle-' + id);
      
      if (desc.classList.contains('expanded')) {
        desc.classList.remove('expanded');
        toggle.textContent = '[show abstract]';
      } else {
        desc.classList.add('expanded');
        toggle.textContent = '[hide abstract]';
      }
    }
    
    function toggleYear(year) {
      const header = document.getElementById('year-header-' + year);
      const content = document.getElementById('year-content-' + year);
      
      header.classList.toggle('collapsed');
      content.classList.toggle('collapsed');
    }
  </script>
  
  <!-- Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-E4HH3XJ50W"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-E4HH3XJ50W', {
      'anonymize_ip': false,  // Set to true if you want to anonymize IP addresses
      'allow_google_signals': true,  // Enables Demographics and Interests reports
      'allow_ad_personalization_signals': false  // Disable ad personalization
    });
  </script>
</head>

<body>
  <!-- Sticky Navigation Bar -->
  <nav class="navbar">
    <div class="navbar-container">
      <a href="#about">Ishan Dave</a>
      <a href="#updates">Updates</a>
      <a href="#experience">Experience</a>
      <a href="#research">Publications</a>
      <a href="#patents">Patents</a>
      <a href="#recognition">Recognition</a>
    </div>
  </nav>

  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table id="about" style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Ishan Dave</name>
              </p>
              <p style="font-size:15px">I am a full-time Applied Scientist at Adobe Firefly working on image and video diffusion models. I finished my Ph.D. in in 2024, from the Center for Research in Computer Vision (CRCV), University of Central Florida (UCF), advised by <a href="https://www.crcv.ucf.edu/person/mubarak-shah/" style="font-size:15px">Prof. Mubarak Shah</a>. My PhD dissertation was about Video Understanding and Self-Supervised Learning.
			
				<!-- Before that I worked as fulltime research assistant at RBCCPS of Indian Institute of Science, Bangalore, India and Indian Institute of Technology, Indore. My undergrad was from National Institute of Technology, Surat, India. -->

			
              </p>
              <p style="text-align:center">
                <img src="https://upload.wikimedia.org/wikipedia/commons/4/4e/Mail_%28iOS%29.svg" alt="Email" style="width:20px;height:20px;vertical-align:middle;margin-right:5px;">
                Email: ishandave95(a)gmail.com
              </p>
              <p style="text-align:center">
                <a href="https://scholar.google.com/citations?hl=en&user=fWu6sFgAAAAJ&view_op=list_works&sortby=pubdate">
                  <img src="https://upload.wikimedia.org/wikipedia/commons/c/c7/Google_Scholar_logo.svg" alt="Google Scholar" style="width:20px;height:20px;vertical-align:middle;margin-right:5px;">Google Scholar
                </a> &nbsp/&nbsp
                <a href="https://github.com/daveishan">
                  <img src="https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png" alt="Github" style="width:20px;height:20px;vertical-align:middle;margin-right:5px;">Github
                </a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/ishan-dave-crcv/">
                  <img src="https://upload.wikimedia.org/wikipedia/commons/c/ca/LinkedIn_logo_initials.png" alt="LinkedIn" style="width:20px;height:20px;vertical-align:middle;margin-right:5px;">LinkedIn
                </a>
              </p>
              <p style="text-align:center;margin-top:10px;">
                <a href="data/CVpublic_Ishan_17Dec2025.pdf">
                  <img src="https://upload.wikimedia.org/wikipedia/commons/8/87/PDF_file_icon.svg" alt="CV" style="width:20px;height:20px;vertical-align:middle;margin-right:5px;">CV
                </a>
                <span style="font-size:11px;color:#888;"> (public version, for more details feel free to ask!)</span>
                <br>
                <span style="font-size:11px;color:#999;">Last updated: Dec 17, 2025</span>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/ishan_pic-modified.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/ishan_pic-modified.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

	<table id="updates"
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
<!--                   <h2>Updates <strong><font color="red">Looking for fulltime jobs!</font></strong> </h2> -->
		  <h2>Updates</h2>
		  
		  <div class="year-divider" id="year-header-2025" onclick="toggleYear('2025')">2025</div>
		  <div class="year-updates" id="year-content-2025">
		    <span style="font-family: 'Courier New', Courier, monospace; color: #696969;">&nbsp;<i class="fa fa-share-alt" style="font-size:12px"></i> Dec'25:</span> A <strong>First author work</strong> "CreativeVR" released: state-of-the-art video restoration model <br>
		    <span style="font-family: 'Courier New', Courier, monospace; color: #696969;">&nbsp;<i class="fa fa-share-alt" style="font-size:12px"></i> Oct'25:</span> My led work "Generative Upscaler" is shipped as the default upscaler in the Photoshop üí•<br>
		    <span style="font-family: 'Courier New', Courier, monospace; color: #696969;">&nbsp;<i class="fa fa-share-alt" style="font-size:12px"></i> Sep'25:</span> A paper "Finegrained Video Retrieval" accepted at <strong>NeurIPS 2025</strong><br>
		    <span style="font-family: 'Courier New', Courier, monospace; color: #696969;">&nbsp;<i class="fa fa-share-alt" style="font-size:12px"></i> Jun'25:</span> A paper "GT-Loc" accepted at <strong>ICCV 2025</strong>: <font color="red">Oral presentation! (Top 0.5% papers)</font><br>
		    <span style="font-family: 'Courier New', Courier, monospace; color: #696969;">&nbsp;<i class="fa fa-share-alt" style="font-size:12px"></i> Apr'25:</span> A paper "ALBAR" accepted at <strong>ICLR 2025</strong><br>
		    <span style="font-family: 'Courier New', Courier, monospace; color: #696969;">&nbsp;<i class="fa fa-share-alt" style="font-size:12px"></i> Jan'25:</span> Started full-time at Adobe Firefly, Seattle, WA<br>
		  </div>
		  
		  <div class="year-divider" id="year-header-2024" onclick="toggleYear('2024')">2024</div>
		  <div class="year-updates" id="year-content-2024">
		    <span style="font-family: 'Courier New', Courier, monospace; color: #696969;">&nbsp;<i class="fa fa-share-alt" style="font-size:12px"></i> Oct'24:</span> Successfully defended my Ph.D dissertation! <br>
		    <span style="font-family: 'Courier New', Courier, monospace; color: #696969;">&nbsp;<i class="fa fa-share-alt" style="font-size:12px"></i> Aug'24:</span> SPAct Patent Approved! my first patent as the primary inventor üí•<br>
		    <span style="font-family: 'Courier New', Courier, monospace; color: #696969;">&nbsp;<i class="fa fa-share-alt" style="font-size:12px"></i> Jul'24:</span> <strong>2 First author papers</strong> accepted at <strong>ECCV 2024</strong>: <font color="red">Oral presentation! (Top 3% papers)</font>üí•üí•<br>
		    <span style="font-family: 'Courier New', Courier, monospace; color: #696969;">&nbsp;<i class="fa fa-share-alt" style="font-size:12px"></i> Jun'24:</span> Selected as Outstanding Reviewer of CVPR 2024! (top 2% among 10,000 reviewers)ü•á<br>
		    <span style="font-family: 'Courier New', Courier, monospace; color: #696969;">&nbsp;<i class="fa fa-share-alt" style="font-size:12px"></i> May'24:</span> Started internship at Apple, Cupertino, CA<br>
		  </div>
		  
		  <div class="year-divider" id="year-header-2023" onclick="toggleYear('2023')">2023</div>
		  <div class="year-updates" id="year-content-2023">
		    <span style="font-family: 'Courier New', Courier, monospace; color: #696969;">&nbsp;<i class="fa fa-share-alt" style="font-size:12px"></i> Dec'23:</span> A <strong>First author paper</strong> "No More Shortcuts" accepted to <strong>AAAI 2024</strong> üí•<br>
		    <span style="font-family: 'Courier New', Courier, monospace; color: #696969;">&nbsp;<i class="fa fa-share-alt" style="font-size:12px"></i> Jul'23:</span> A <strong>First author paper</strong> "Event-TransAct" accepted at IROS 2023 üí•<br>
		    <span style="font-family: 'Courier New', Courier, monospace; color: #696969;">&nbsp;<i class="fa fa-share-alt" style="font-size:12px"></i> Jul'23:</span> A paper "TeD-SPAD" accepted at ICCV 2023<br>
		    <span style="font-family: 'Courier New', Courier, monospace; color: #696969;">&nbsp;<i class="fa fa-share-alt" style="font-size:12px"></i> May'23:</span> Started summer internship at Adobe, San Jose, CA<br>
		    <span style="font-family: 'Courier New', Courier, monospace; color: #696969;">&nbsp;<i class="fa fa-share-alt" style="font-size:12px"></i> Mar'23:</span> A <strong>First author paper</strong> "TimeBalance" accepted to <strong>CVPR 2023</strong> üí•<br>
		    <span style="font-family: 'Courier New', Courier, monospace; color: #696969;">&nbsp;<i class="fa fa-share-alt" style="font-size:12px"></i> Jan'23:</span> A paper "TransVisDrone" accepted at ICRA 2023<br>
		  </div>
		  
		  <div class="year-divider" id="year-header-2022" onclick="toggleYear('2022')">2022</div>
		  <div class="year-updates" id="year-content-2022">
		    <span style="font-family: 'Courier New', Courier, monospace; color: #696969;">&nbsp;<i class="fa fa-share-alt" style="font-size:12px"></i> May'22:</span> Started summer internship at Adobe, USA (remote- Florida)<br>
		    <span style="font-family: 'Courier New', Courier, monospace; color: #696969;">&nbsp;<i class="fa fa-share-alt" style="font-size:12px"></i> Mar'22:</span> A <strong>First author paper</strong> "TCLR" accepted to CVIU 2022 üí•<br>
		    <span style="font-family: 'Courier New', Courier, monospace; color: #696969;">&nbsp;<i class="fa fa-share-alt" style="font-size:12px"></i> Mar'22:</span> A <strong>First author paper</strong> "SPAct" accepted to <strong>CVPR 2022</strong> üí•<br>
		  </div>
		  
		  <div class="year-divider" id="year-header-2021" onclick="toggleYear('2021')">2021 & Earlier</div>
		  <div class="year-updates" id="year-content-2021">
		    <span style="font-family: 'Courier New', Courier, monospace; color: #696969;">&nbsp;<i class="fa fa-share-alt" style="font-size:12px"></i> Jan'21:</span> Our Gabriella paper has been awarded the best scientific paper award at ICPR 2020<br>
		  </div>
<!--                     &nbsp <i class="fa fa-share-alt" style="font-size:12px"></i> June 2020: Placed first at ActEV SDL Challenge (ActivityNet workshop at CVPR 2020) ü•á<br>
                    &nbsp <i class="fa fa-share-alt" style="font-size:12px"></i> October 2019: Placed second at the TRECVID leaderboard ü•á<br> -->
                </td>
              </tr>
            </tbody>
          </table>
	
        <table id="experience" width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Work Experience</heading>
            </td>
          </tr>
          </table>
        
	<table width="100%" align="center" border="0" cellpadding="20"></tbody>

            <tr>
              <td style="padding:10px;width:25%;vertical-align:top">
		<img src='images/Adobe-logo.png' style="width:100%;">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <strong>Applied Scientist</strong>
                <br> Adobe Inc., Seattle, Washington, USA. Jan 2025 - Present
                <p></p>
                <ul style="margin: 0; padding-left: 20px; line-height: 1.2;">
		    <li>Working on image and video diffusion models across open research and in-house for Adobe Firefly.</li>
		    <li><strong>Initiated and led</strong> the <em>Generative Image Upscaler</em> project end-to-end: from proposing the idea and developing the research prototype to training the in-house production model and driving performance improvements beyond competitive systems. Generative Upscaler shipped in general-release of <em><a href="https://helpx.adobe.com/photoshop/desktop/repair-retouch/clean-restore-images/enhance-image-quality-with-generative-upscale.html" target="_blank">Photoshop</a></em> as the <strong>default</strong> upsampling solution and a core component of Adobe's flagship <em><a href="https://business.adobe.com/products/firefly-business/custom-models.html" target="_blank">Firefly Custom Models</a></em> ecosystem.</li>
			<li> <strong></strong>Creative Video Restoration:</strong> Developed a state-of-the-art video restoration model for AI-generated and real videos with severe structural and temporal artifacts.</li>
		</ul>	      
              </td>
            </tr>
          </tr>
          </table>

	<table width="100%" align="center" border="0" cellpadding="20"></tbody>

            <tr>
              <td style="padding:10px;width:25%;vertical-align:top; text-align:center">
		<img src='images/apple_logo.png' style="width:50%;">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <strong>PhD AI/ML Intern</strong>
                <br> Apple Inc., Cupertino, California, USA. May 2024- Aug 2024
	        <p></p>
		<ul style="margin: 0; padding-left: 20px; line-height: 1.2;">
		    <li>Enhanced stable diffusion models for image editing by leveraging vision-language multimodal foundation models.</li>
		    <li>Trained diffusion models on a large-scale, high-resolution dataset of 10M samples.</li>
		    <li>Reproduced and outperformed state-of-the-art image editing methods using a novel approach, achieving superior results.</li>
		</ul>	      
              </td>
		<p></p>
              </td>
            </tr>
          </tr>
          </table>
	
          <table width="100%" align="center" border="0" cellpadding="20"></tbody>

            <tr>
              <td style="padding:10px;width:25%;vertical-align:top">
		<img src='images/Adobe-logo.png' style="width:100%;">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <strong>Research Scientist/ Engineer Intern</strong>
                <br> Adobe Inc., San Jose, California, USA. May 2023- Nov 2023
                <br> Host: <a href="https://sjenni.github.io/">Simon Jenni, </a>
                <a href="https://fabiancaba.com/">Fabian Caba</a>
                <p></p>
                <ul style="margin: 0; padding-left: 20px; line-height: 1.2;">
		    <li>Enhanced the fine-grained capabilities of existing video retrieval methods.</li>
		    <li>Worked on large-scale video galleries with millions of samples.</li>
		    <li>Filed a patent and had a paper accepted at ECCV 2024.</li>
		</ul>	      
              </td>
            </tr>
          </tr>
          </table>
	 <table width="100%" align="center" border="0" cellpadding="20"></tbody>

            <tr>
               <td style="padding:10px;width:25%;vertical-align:top">
		<img src='images/Adobe-logo.png' style="width:100%;">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <strong>Research Scientist Intern</strong>
                <br> Adobe Inc., Remote, USA. May 2022 - Nov 2022
                <br> Host: <a href="https://sjenni.github.io/">Simon Jenni</a>
                <p></p>
                <ul style="margin: 0; padding-left: 20px; line-height: 1.2;">
		    <li>Developed a novel self-supervised video representation framework by reformulating temporal self-supervision as frame-level recognition tasks and introducing an effective augmentation strategy to mitigate shortcuts.</li>
		    <li>Achieved state-of-the-art performance on 10 video understanding benchmarks across linear classification (Kinetics400, HVU, SSv2, Charades), video retrieval (UCF101, HMDB51), and temporal correspondence (CASIA-B).</li>
		    <li>Published a paper at AAAI 2024.</li>
		</ul>	      
              </td>
            </tr>
          </tr>
          </table>

         

        <table id="research" style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Publications</heading>
              <p>
                I have a broad interest in computer vision and machine learning. My primary research focuses on <strong>video understanding</strong>: self/semi supervised learning and action recognition.  
<!-- 		      My current research mainly focuses on video representation learning with limited labels (self/semi-supervised learning), action recognition, and privacy preservation in video understanding tasks.  -->
		      My recent research also includes enhancing the fine-grained video understanding of the large foundational models and improving multi-modal generative AI for image editing applications. I have also worked on various robotics-related vision tasks like event-camera-based action recognition and drone-to-drone detections from videos. 
                <br> Below is a selected list of my works (in <strong>chronological order</strong>), representative papers are <span style="background-color: #ffffd0;">highlighted</span>.

              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

	<!-- ArXiv 2025 - CreativeVR -->
	<tr onmouseout="mira_stop()" onmouseover="mira_start()" bgcolor="#ffffd0">
	    <td style="padding:20px;width:25%;vertical-align:top;text-align:center">
		<div class="one">
		    <img src='images/creativevr_logo.jpg' width="126">
		</div>
	    </td>
	    <td style="padding:20px;width:75%;vertical-align:top">
		<a href="https://arxiv.org/abs/2512.12060">
		    <papertitle>CreativeVR: Diffusion-Prior-Guided Approach for Structure and Motion Restoration in Generative and Real Videos</papertitle>
		</a>
		<br>
		<strong>Ishan Rajendrakumar Dave*</strong>,
		<a href="https://tejaspanambur.github.io/">Tejas Panambur*</a>,
		<a href="https://chongjiange.github.io/">Chongjian Ge</a>,
		<a href="https://www.linkedin.com/in/meyumer/">Ersin Yumer</a>,
		<a href="https://sites.google.com/view/xuebai/home">Xue Bai</a>
		<br>
		<em>ArXiv Preprint</em>, 2025
		<br>
		*= equal contribution
		<br>
		<div class="paper-links">
		  <a href="https://arxiv.org/pdf/2512.12060" class="paper-link-btn">PDF</a>
		  <a href="https://daveishan.github.io/creativevr-webpage/" class="paper-link-btn">Website</a>
		  <a class="toggle-description" id="toggle-desc-creativevr" onclick="toggleDescription('desc-creativevr')">[show abstract]</a>
		</div>
		<div id="desc-creativevr" class="paper-description">
		<p>
			Modern text-to-video (T2V) diffusion models can synthesize visually compelling clips, yet they remain brittle at fine-scale structure: even state-of-the-art generators often produce distorted faces and hands, warped backgrounds, and temporally inconsistent motion. Such severe structural artifacts also appear in very low-quality real-world videos. Classical video restoration and super-resolution (VR/VSR) methods, in contrast, are tuned for synthetic degradations such as blur and downsampling and tend to stabilize these artifacts rather than repair them, while diffusion-prior restorers are usually trained on photometric noise and offer little control over the trade-off between perceptual quality and fidelity.
			<br><br>
			We introduce CreativeVR, a diffusion-prior-guided video restoration framework for AI-generated (AIGC) and real videos with severe structural and temporal artifacts. Our deep-adapter-based method exposes a single precision knob that controls how strongly the model follows the input, smoothly trading off between precise restoration on standard degradations and stronger structure- and motion-corrective behavior on challenging content. Our key novelty is a temporally coherent degradation module used during training, which applies carefully designed transformations that produce realistic structural failures.
			<br><br>
			To evaluate AIGC-artifact restoration, we propose the AIGC54 benchmark with FIQA, semantic and perceptual metrics, and multi-aspect scoring. CreativeVR achieves state-of-the-art results on videos with severe artifacts and performs competitively on standard video restoration benchmarks, while running at practical throughput (~13 FPS @ 720p on a single 80 GB A100).
		</p>
		</div>
	     </td>
	</tr>

	<!-- NeurIPS 2025 - TF-CoVR -->
	<tr onmouseout="mira_stop()" onmouseover="mira_start()">
	    <td style="padding:20px;width:25%;vertical-align:top">
		<div class="one">
		    <img src='images/tfcovr_neurips2025.png' width="180">
		</div>
	    </td>
	    <td style="padding:20px;width:75%;vertical-align:top">
		<a href="https://arxiv.org/pdf/2506.05274">
		    <papertitle>From Play to Replay: Composed Video Retrieval for Temporally Fine-Grained Videos</papertitle>
		</a>
		<br>
		<a href="https://animesh-007.github.io/">Animesh Gupta</a>,
		Jay Parmar,
		<strong>Ishan Rajendrakumar Dave</strong>, 
		<a href="https://www.crcv.ucf.edu/person/mubarak-shah/">Mubarak Shah</a>
		<br>
		<em>Conference on Neural Information Processing Systems (<strong>NeurIPS</strong>) </em>, 2025
		<br>
		<div class="paper-links">
		  <a href="https://arxiv.org/pdf/2506.05274" class="paper-link-btn">PDF</a>
		  <a href="https://animesh-007.github.io/TF-CoVR-WEBSITE/" class="paper-link-btn">Website</a>
		  <a href="https://github.com/UCF-CRCV/TF-CoVR" class="paper-link-btn">Code</a>
		  <a class="toggle-description" id="toggle-desc-tfcovr" onclick="toggleDescription('desc-tfcovr')">[show abstract]</a>
		</div>
		<div id="desc-tfcovr" class="paper-description">
		<p>
			Composed Video Retrieval (CoVR) retrieves a target video given a query video and a modification text describing the intended change. Existing CoVR benchmarks emphasize appearance shifts or coarse event changes and therefore do not test the ability to capture subtle, fast-paced temporal differences. We introduce TF-CoVR, the first large-scale benchmark dedicated to temporally fine-grained CoVR. TF-CoVR focuses on gymnastics and diving and provides 180K triplets drawn from FineGym and FineDiving.
		</p>
		</div>
	     </td>
	</tr>

	<!-- ICCV 2025 - GT-Loc -->
	<tr onmouseout="mira_stop()" onmouseover="mira_start()">
	    <td style="padding:20px;width:25%;vertical-align:top">
		<div class="one">
		    <img src='images/gtloc_iccv2025.png' width="180">
		</div>
	    </td>
	    <td style="padding:20px;width:75%;vertical-align:top">
		<a href="https://arxiv.org/pdf/2507.10473">
		    <papertitle>GT-Loc: Unifying When and Where in Images Through a Joint Embedding Space</papertitle>
		</a>
		<br>
		<a href="https://davidshatwell.com/">David Shatwell</a>,
		<strong>Ishan Rajendrakumar Dave</strong>,
		<a href="https://swetha5.github.io/">Sirnam Swetha</a>,
		<a href="https://www.crcv.ucf.edu/person/mubarak-shah/">Mubarak Shah</a>
		<br>
		<em>International Conference on Computer Vision (<strong>ICCV</strong>) </em>, 2025
		<br>
		<em><strong><font color="red">Oral presentation! (Top 0.6% papers)</font></strong></em>
		<br>
		<div class="paper-links">
		  <a href="https://arxiv.org/pdf/2507.10473" class="paper-link-btn">PDF</a>
		  <a href="https://davidshatwell.com/gtloc.github.io/" class="paper-link-btn">Website</a>
		  <a href="https://iccv.thecvf.com/media/PosterPDFs/ICCV%202025/544.png?t=1759974999.7209623" class="paper-link-btn">Poster</a>
		  <a class="toggle-description" id="toggle-desc-gtloc" onclick="toggleDescription('desc-gtloc')">[show abstract]</a>
		</div>
		<div id="desc-gtloc" class="paper-description">
		<p>
			Timestamp prediction aims to determine when an image was captured using only visual information, supporting applications such as metadata correction, retrieval, and digital forensics. To address the interdependence between time and location, we introduce GT-Loc, a novel retrieval-based method that jointly predicts the capture time (hour and month) and geo-location (GPS coordinates) of an image. Our approach employs separate encoders for images, time, and location, aligning their embeddings within a shared high-dimensional feature space.
		</p>
		</div>
	     </td>
	</tr>

	<!-- ICLR 2025 - ALBAR -->
	<tr onmouseout="mira_stop()" onmouseover="mira_start()">
	    <td style="padding:20px;width:25%;vertical-align:top">
		<div class="one">
		    <img src='images/albar_iclr2025.png' width="180">
		</div>
	    </td>
	    <td style="padding:20px;width:75%;vertical-align:top">
		<a href="https://arxiv.org/pdf/2502.00156">
		    <papertitle>ALBAR: Adversarial Learning approach to mitigate Biases in Action Recognition</papertitle>
		</a>
		<br>
		<a href="https://joefioresi718.github.io/">Joseph Fioresi</a>,
		<strong>Ishan Rajendrakumar Dave</strong>, 
		<a href="https://www.crcv.ucf.edu/person/mubarak-shah/">Mubarak Shah</a>
		<br>
		<em>International Conference on Learning Representations (<strong>ICLR</strong>) </em>, 2025
		<br>
		<em><strong>Poster Presentation</strong></em>
		<br>
		<div class="paper-links">
		  <a href="https://arxiv.org/pdf/2308.11072" class="paper-link-btn">PDF</a>
		  <a href="https://joefioresi718.github.io/ALBAR_webpage/" class="paper-link-btn">Website</a>
		  <a href="https://github.com/UCF-CRCV/ALBAR" class="paper-link-btn">Code</a>
		  <a class="toggle-description" id="toggle-desc-albar" onclick="toggleDescription('desc-albar')">[show abstract]</a>
		</div>
		<div id="desc-albar" class="paper-description">
		<p>
			We propose ALBAR, an adversarial learning approach to mitigate biases in action recognition. Our method addresses the challenge of spurious correlations between visual features and action labels that can lead to biased model predictions.
		</p>
		</div>
	     </td>
	</tr>

	<tr onmouseout="mira_stop()" onmouseover="mira_start()">
	    <td style="padding:20px;width:25%;vertical-align:top">
		<div class="one">
		    <img src='images/avr_eccv2024.png' width="180">
		</div>
	    </td>
	    <td style="padding:20px;width:75%;vertical-align:top">
		<!-- <a href="https://www.crcv.ucf.edu/wp-content/uploads/2018/11/avr_eccv24_dave.pdf"> -->
		<a href="https://arxiv.org/pdf/2409.01445">
		    <papertitle>Sync from the Sea: Retrieving Alignable Videos from Large-Scale Datasets</papertitle>
		</a>
		<br>
		<strong>Ishan Rajendrakumar Dave</strong>, 
		<a href="https://fabiancaba.com/">Fabian Caba</a>, 
		<a href="https://www.crcv.ucf.edu/person/mubarak-shah/">Mubarak Shah</a>, 
		<a href="https://sjenni.github.io/">Simon Jenni</a>.
		<br>
		<em>The 18th European Conference on Computer Vision (<strong>ECCV</strong>) </em>, 2024
		<br>
	       <em><strong><font color="red">Oral presentation! (Top 3% of accepted papers)</font></strong></em>
		<br>
		<div class="paper-links">
		  <a href="https://arxiv.org/pdf/2409.01445" class="paper-link-btn">PDF</a>
		  <a href="https://daveishan.github.io/avr-webpage/" class="paper-link-btn">Website</a>
		  <a class="toggle-description" id="toggle-desc1" onclick="toggleDescription('desc1')">[show abstract]</a>
		</div>
		<div id="desc1" class="paper-description">
		<p>
			Temporal video alignment synchronizes key events like object interactions or action phase transitions in two videos, benefiting video editing, processing, and understanding tasks. Existing methods assume a given video pair, limiting applicability. We redefine this as a search problem, introducing Alignable Video Retrieval (AVR), which identifies and synchronizes well-alignable videos from a large collection. Key contributions include DRAQ, a video alignability indicator, and a generalizable frame-level video feature design.
		</p>
		</div>
	     </td>
	</tr>

	<tr onmouseout="mira_stop()" onmouseover="mira_start()" bgcolor="#ffffd0">	
		<td style="padding:20px;width:25%;vertical-align:top">
			        <div class="one">
			            <img src='images/finepsuedo_eccv2024.png' width="180">
			        </div>
			    </td>
			    <td style="padding:20px;width:75%;vertical-align:top">
		        <!-- <a href="https://www.crcv.ucf.edu/wp-content/uploads/2018/11/finepsuedo_eccv24_dave.pdf"> -->
				<a href="https://arxiv.org/pdf/2409.01448">
		            <papertitle>FinePseudo: Improving Pseudo-Labelling through Temporal-Alignablity for Semi-Supervised Fine-Grained Action Recognition</papertitle>
		        </a>
		        <br>
		        <strong>Ishan Rajendrakumar Dave</strong>, 
		        <a href="https://nayeemrizve.github.io/">Mamshad Nayeem Rizve</a>, 
		        <a href="https://www.crcv.ucf.edu/person/mubarak-shah/">Mubarak Shah</a>.
		        <br>
		        <em>The 18th European Conference on Computer Vision (<strong>ECCV</strong>) </em>, 2024
		        <br>
			<div class="paper-links">
			  <a href="https://arxiv.org/pdf/2409.01448" class="paper-link-btn">PDF</a>
			  <a href="https://daveishan.github.io/finepsuedo-webpage/" class="paper-link-btn">Website</a>
			  <a class="toggle-description" id="toggle-desc2" onclick="toggleDescription('desc2')">[show abstract]</a>
			</div>
			<div id="desc2" class="paper-description">
			<p>
				We introduce Alignability-Verification-based Metric learning for semi-supervised fine-grained action recognition. Using dynamic time warping (DTW) for action-phase-aware comparison, our learnable alignability score refines pseudo-labels of the video encoder. Our framework, FinePseudo, outperforms prior methods on fine-grained action recognition datasets. Additionally, it demonstrates robustness in handling novel unlabeled classes in open-world setups.
			</p>
			</div>
		    	</td>
	</tr>

		
<!-- 	<tr onmouseout="mira_stop()" onmouseover="mira_start()" bgcolor="#ffffd0"> -->
		
	    <td style="padding:20px;width:25%;vertical-align:top">
	        <div class="one">
	            <img src='images/codamal_icip2024.png' width="180">
	        </div>
	    </td>
	    <td style="padding:20px;width:75%;vertical-align:top">
        <a href="https://arxiv.org/pdf/2312.13008.pdf">
            <papertitle>CodaMal: Contrastive Domain Adaptation for Malaria Detection in Low-Cost Microscopes</papertitle>
        </a>
        <br>
        <strong>Ishan Rajendrakumar Dave</strong>, 
	<a href="https://github.com/tristandb8">Tristan de Blegiers</a>, 
	<a href="https://www.crcv.ucf.edu/chenchen/">Chen Chen</a>,
        <a href="https://www.crcv.ucf.edu/person/mubarak-shah/">Mubarak Shah</a>.
        <br>
        <em>31st IEEE International Conference on Image Processing (<strong>ICIP</strong>) </em>, 2024
        <br>
        <em><strong><font color="red">Oral presentation! </font></strong></em>
	<br>
	<div class="paper-links">
	  <a href="https://arxiv.org/pdf/2312.13008.pdf" class="paper-link-btn">PDF</a>
	  <a href="https://daveishan.github.io/codamal-webpage/" class="paper-link-btn">Website</a>
	  <a href="https://github.com/DAVEISHAN/CodaMal" class="paper-link-btn">Code</a>
	  <a class="toggle-description" id="toggle-desc3" onclick="toggleDescription('desc3')">[show abstract]</a>
	</div>
	<div id="desc3" class="paper-description">
	<p>
		We propose a Domain Adaptive Contrastive objective to bridge the gap between High and Low Cost Microscopes. On the publicly available large-scale M5 dataset, our proposed method shows a significant improvement of 16% over the state-of-the-art methods in terms of the mean average precision metric (mAP), provides a 21√ó speed-up during inference, and requires only half as many learnable parameters as the prior methods.
	</p>
	</div>
    	</td>
<!-- 	</tr> -->



		
	<tr onmouseout="mira_stop()" onmouseover="mira_start()" bgcolor="#ffffd0">
	    <td style="padding:20px;width:25%;vertical-align:top">
	        <div class="one">
	            <img src='images/nms_aaai2024.png' width="180">
	        </div>
	    </td>
	    <td style="padding:20px;width:75%;vertical-align:top">
        <a href="https://arxiv.org/pdf/2312.13008.pdf">
            <papertitle>No More Shortcuts: Realizing the Potential of Temporal Self-Supervision</papertitle>
        </a>
        <br>
        <strong>Ishan Rajendrakumar Dave</strong>, 
        <a href="https://sjenni.github.io/">Simon Jenni</a>,  
        <a href="https://www.crcv.ucf.edu/person/mubarak-shah/">Mubarak Shah</a>.
        <br>
        <em>AAAI Conference on Artificial Intelligence, Main Technical Track (<strong>AAAI</strong>) </em>, 2024
        <br>
	<div class="paper-links">
	  <a href="https://arxiv.org/pdf/2312.13008.pdf" class="paper-link-btn">PDF</a>
	  <a href="https://daveishan.github.io/nms-webpage/" class="paper-link-btn">Website</a>
	  <a class="toggle-description" id="toggle-desc4" onclick="toggleDescription('desc4')">[show abstract]</a>
	</div>
	<div id="desc4" class="paper-description">
	<p>
		We demonstrate experimentally that our more challenging frame-level task formulations and the removal of shortcuts drastically improve the quality of features learned through temporal self-supervision. Our extensive experiments show state-of-the-art performance across 10 video understanding datasets, illustrating the generalization ability and robustness of our learned video representations.
	</p>
	</div>
    	</td>
	</tr>
		
		
	<tr onmouseout="mira_stop()" onmouseover="mira_start()">
	    <td style="padding:20px;width:25%;vertical-align:top">
		<div class="one">
		    <img src='images/tedspad_iccv2023.png' width="180">
		</div>
	    </td>
	    <td style="padding:20px;width:75%;vertical-align:top">
		<a href="LINK_TO_PAPER">
		    <papertitle>TeD-SPAD: Temporal Distinctiveness for Self-supervised Privacy-preservation for Video Anomaly Detection</papertitle>
		</a>
		<br>
		<a href="https://joefioresi718.github.io/">Joseph Fioresi</a>, 
		<strong>Ishan Rajendrakumar Dave</strong>, 
		<a href="https://www.crcv.ucf.edu/person/mubarak-shah/">Mubarak Shah</a>.
		<br>
		<em>Proceedings of the IEEE/CVF International Conference on Computer Vision (<strong>ICCV</strong>)</em>, 2023
		<br>
		<div class="paper-links">
		  <a href="https://arxiv.org/pdf/2308.11072.pdf" class="paper-link-btn">PDF</a>
		  <a href="https://joefioresi718.github.io/TeD-SPAD_webpage/" class="paper-link-btn">Website</a>
		  <a href="https://github.com/UCF-CRCV/TeD-SPAD" class="paper-link-btn">Code</a>
		  <a class="toggle-description" id="toggle-desc5" onclick="toggleDescription('desc5')">[show abstract]</a>
		</div>
		<div id="desc5" class="paper-description">
		<p>
		    We propose TeD-SPAD, a privacy-aware video anomaly detection framework that destroys visual private information in a self-supervised manner. In particular, we propose the use of a temporally-distinct triplet loss to promote temporally discriminative features, which complements current weakly-supervised VAD methods.
		</p>
		</div>
	    </td>
	</tr>

	<tr onmouseout="mira_stop()" onmouseover="mira_start()">
	    <td style="padding:20px;width:25%;vertical-align:top">
	        <div class="one">
	            <img src='images/eventtransact_iros2023.png' width="180">
	        </div>
	    </td>
	    <td style="padding:20px;width:75%;vertical-align:top">
	        <a href="https://arxiv.org/pdf/2308.13711">
	            <papertitle>EventTransAct: A Video Transformer-based Framework for Event-camera Based Action Recognition</papertitle>
	        </a>
	        <br>
	        <a href="https://github.com/tristandb8">Tristan de Blegiers</a>*, 
	        <strong>Ishan Rajendrakumar Dave*</strong>, 
	        Adeel Yousaf, 
	        <a href="https://www.crcv.ucf.edu/person/mubarak-shah/">Mubarak Shah</a>.
		<br>
		*= equal contribution
	        <br>
	        <em>IEEE/RSJ International Conference on Intelligent Robots and Systems (<strong>IROS</strong>)</em>, 2023
	        <br>
		<div class="paper-links">
		  <a href="https://arxiv.org/pdf/2308.13711" class="paper-link-btn">PDF</a>
		  <a href="https://github.com/tristandb8/EventTransAct" class="paper-link-btn">Code</a>
		  <a class="toggle-description" id="toggle-desc6" onclick="toggleDescription('desc6')">[show abstract]</a>
		</div>
		<div id="desc6" class="paper-description">
		<p>
		   We propose a video transformer-based framework for event-camera based action recognition, which leverages event-contrastive loss and augmentations to adapt the network to event data. Our method achieved state-of-the-art results on N-EPIC Kitchens dataset and competitive results on the standard DVS Gesture recognition dataset, while requiring less computation time compared to competitive prior approaches. 
		</p>
		</div>
	    </td>
	</tr>

		
		
          <tr onmouseout="mira_stop()" onmouseover="mira_start()" bgcolor="#ffffd0">
	    <td style="padding:20px;width:25%;vertical-align:top">
	        <div class="one">
	            <img src='images/timebalance_cvpr2023.png' width="180">
	        </div>
	    </td>
	    <td style="padding:20px;width:75%;vertical-align:top">
        <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Dave_TimeBalance_Temporally-Invariant_and_Temporally-Distinctive_Video_Representations_for_Semi-Supervised_Action_Recognition_CVPR_2023_paper.pdf">
            <papertitle>TimeBalance: Temporally-Invariant and Temporally-Distinctive Video Representations for Semi-Supervised Action Recognition</papertitle>
        </a>
        <br>
        <strong>Ishan Rajendrakumar Dave</strong>, 
        <a href="https://nayeemrizve.github.io/">Mamshad Nayeem Rizve</a>, 
        <a href="https://www.crcv.ucf.edu/chenchen/">Chen Chen</a>, 
        <a href="https://www.crcv.ucf.edu/person/mubarak-shah/">Mubarak Shah</a>.
        <br>
        <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2023
        <br>
	<div class="paper-links">
	  <a href="https://arxiv.org/pdf/2303.16268.pdf" class="paper-link-btn">PDF</a>
	  <a href="https://daveishan.github.io/timebalance_webpage/" class="paper-link-btn">Website</a>
	  <a href="https://github.com/DAVEISHAN/TimeBalance" class="paper-link-btn">Code</a>
	  <a class="toggle-description" id="toggle-desc7" onclick="toggleDescription('desc7')">[show abstract]</a>
	</div>
	<div id="desc7" class="paper-description">
	<p>
		We propose a student-teacher semi-supervised learning framework, where we distill knowledge from a temporally-invariant and temporally-distinctive teacher. Depending on the nature of the unlabeled video, we dynamically combine the knowledge of these two teachers based on a novel temporal similarity-based reweighting scheme. State-of-the-art results on Kinetics400, UCF101, HMDB51.
	</p>
	</div>
    	</td>
	</tr>

	<tr onmouseout="mira_stop()" onmouseover="mira_start()">
	    <td style="padding:20px;width:25%;vertical-align:top">
	        <div class="one">
	            <img src='images/transvisdrone_icra23.png' width="180">
	        </div>
	    </td>
	    <td style="padding:20px;width:75%;vertical-align:top">
	        <a href="https://ieeexplore.ieee.org/document/10161433">
	            <papertitle>Transvisdrone: Spatio-temporal Transformer for Vision-based Drone-to-drone Detection in Aerial Videos</papertitle>
	        </a>
	        <br>
	        Tushar Sangam, 
	        <strong>Ishan Rajendrakumar Dave</strong>,
	        <a href="https://waqassultani.github.io/">Waqas Sultani</a>, 
	        <a href="https://www.crcv.ucf.edu/person/mubarak-shah/">Mubarak Shah</a>.
	        <br>
	        <em>2023 IEEE International Conference on Robotics and Automation (<strong>ICRA</strong>)</em>, 2023
	        <br>
		<div class="paper-links">
		  <a href="https://arxiv.org/pdf/2210.08423.pdf" class="paper-link-btn">PDF</a>
		  <a href="https://tusharsangam.github.io/TransVisDrone-project-page/" class="paper-link-btn">Website</a>
		  <a href="https://github.com/tusharsangam/TransVisDrone" class="paper-link-btn">Code</a>
		  <a class="toggle-description" id="toggle-desc8" onclick="toggleDescription('desc8')">[show abstract]</a>
		</div>
		<div id="desc8" class="paper-description">
		<p>
		    We propose a simple yet effective framework, TransVisDrone, that provides an end-to-end solution with higher computational efficiency. We utilize CSPDarkNet-53 network to learn object-related spatial features and VideoSwin model to improve drone detection in challenging scenarios by learning spatio-temporal dependencies of drone motion.
		</p>
		</div>
	    </td>
	</tr>

	

	<tr onmouseout="mira_stop()" onmouseover="mira_start()" bgcolor="#ffffd0">
	    <td style="padding:20px;width:25%;vertical-align:top">
	        <div class="one">
	            <img src='images/spact_cvpr2022.png' width="180">
	        </div>
	    </td>
	    <td style="padding:20px;width:75%;vertical-align:top">
	        <a href="https://openaccess.thecvf.com/content/CVPR2022/html/Dave_SPAct_Self-Supervised_Privacy_Preservation_for_Action_Recognition_CVPR_2022_paper.html">
	            <papertitle>SPAct: Self-supervised Privacy Preservation for Action Recognition</papertitle>
	        </a>
	        <br>
	        <strong>Ishan Rajendrakumar Dave</strong>, 
	        <a href="https://www.crcv.ucf.edu/chenchen/">Chen Chen</a>, 
	        <a href="https://www.crcv.ucf.edu/person/mubarak-shah/">Mubarak Shah</a>.
	        <br>
	        <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2022
	        <br>
		<div class="paper-links">
		  <a href="https://arxiv.org/pdf/2203.15205.pdf" class="paper-link-btn">PDF</a>
		  <a href="https://github.com/DAVEISHAN/SPAct" class="paper-link-btn">Code</a>
		  <a class="toggle-description" id="toggle-desc9" onclick="toggleDescription('desc9')">[show abstract]</a>
		</div>
		<div id="desc9" class="paper-description">
		<p>
		    For the first time, we present a novel training framework that removes privacy information from input video in a self-supervised manner without requiring privacy labels. We train our framework using a minimax optimization strategy to minimize the action recognition cost function and maximize the privacy cost function through a contrastive self-supervised loss. 
		</p>
		</div>
	    </td>
	</tr>

	

	
		
	<tr onmouseout="mira_stop()" onmouseover="mira_start()" bgcolor="#ffffd0">
	    <td style="padding:5px;width:25%;vertical-align:top">
	        <div class="one">
	            <img src='images/tclr_cviu2022.png' width="180">
	        </div>
	        <script type="text/javascript">
	            function mira_start() {
	                document.getElementById('mira_image').style.opacity = "1";
	            }
	
	            function mira_stop() {
	                document.getElementById('mira_image').style.opacity = "0";
	            }
	            mira_stop()
	        </script>
	    </td>
	    <td style="padding:20px;width:75%;vertical-align:top">
	        <a href="https://www.sciencedirect.com/science/article/pii/S1077314222000376">
	            <papertitle>TCLR: Temporal Contrastive Learning for Video Representation</papertitle>
	        </a>
	        <br>
	        <strong>Ishan Dave</strong>,
	        <a href="https://www.rohitg.xyz/">Rohit Gupta</a>, 
	        <a href="https://nayeemrizve.github.io/">Mamshad Nayeem Rizve</a>, 
	        <a href="https://www.crcv.ucf.edu/person/mubarak-shah/">Mubarak Shah</a>.
	        <br>
	        <em>Computer Vision and Image Understanding (<strong>CVIU</strong>)</em>, 2022
	        <br>
	       <em><strong><font color="red">(250+ citations, Among the top-10 most downloaded papers in CVIU)</font></strong></em>
               <br>
		<div class="paper-links">
		  <a href="https://arxiv.org/pdf/2101.07974.pdf" class="paper-link-btn">PDF</a>
		  <a href="https://github.com/DAVEISHAN/TCLR" class="paper-link-btn">Code</a>
		  <a class="toggle-description" id="toggle-desc10" onclick="toggleDescription('desc10')">[show abstract]</a>
		</div>
		<div id="desc10" class="paper-description">
	        <p>
	            We propose a new temporal contrastive learning framework for self-supervised video representation learning, consisting of two novel losses that aim to increase the temporal diversity of learned features. The framework achieves state-of-the-art results on various downstream video understanding tasks, including significant improvement in fine-grained action classification for visually similar classes.
	        </p>
		</div>
	    </td>
	</tr>

	<tr onmouseout="mira_stop()" onmouseover="mira_start()">
	    <td style="padding:20px;width:25%;vertical-align:top">
	        <div class="one">
	            <img src='images/gabriellav2_wacv22.png' width="180">
	        </div>
	    </td>
	    <td style="padding:20px;width:75%;vertical-align:top">
	        <a href="https://openaccess.thecvf.com/content/WACV2022W/HADCV/papers/Dave_GabriellaV2_Towards_Better_Generalization_in_Surveillance_Videos_for_Action_Detection_WACVW_2022_paper.pdf">
	            <papertitle>Gabriellav2: Towards Better Generalization in Surveillance Videos for Action Detection</papertitle>
	        </a>
	        <br>
	        <strong>Ishan Dave</strong>, 
	        Zacchaeus Scheffer, 
	        <a href="https://akash2907.github.io/">Akash Kumar</a>, 
	        Sarah Shiraz, 
	        <a href="https://www.crcv.ucf.edu/person/rawat/">Yogesh Singh Rawat</a>, 
	        <a href="https://www.crcv.ucf.edu/person/mubarak-shah/">Mubarak Shah</a>.
	        <br>
	        <em>Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (<strong>WACV</strong>)</em>, 2022
	        <br>
		<div class="paper-links">
		  <a href="https://openaccess.thecvf.com/content/WACV2022W/HADCV/papers/Dave_GabriellaV2_Towards_Better_Generalization_in_Surveillance_Videos_for_Action_Detection_WACVW_2022_paper.pdf" class="paper-link-btn">PDF</a>
		  <a class="toggle-description" id="toggle-desc11" onclick="toggleDescription('desc11')">[show abstract]</a>
		</div>
		<div id="desc11" class="paper-description">
	        <p>
	            We propose a realtime, online, action detection system which can generalize robustly on any unknown facility surveillance videos. We tackle the
challenging nature of action classification problem in various aspects like handling the class-imbalance training using PLM method and learning multi-label action correlations using LSEP loss. In order to improve the computational efficiency of the system, we utilize knowledge distillation.
	        </p>
		</div>
	    </td>
	</tr>	
	<tr onmouseout="mira_stop()" onmouseover="mira_start()">
	    <td style="padding:20px;width:25%;vertical-align:top">
	        <div class="one">
	            <img src='images/gabriella_icpr2020.png' width="180">
	        </div>
	    </td>
	    <td style="padding:20px;width:75%;vertical-align:top">
	        <a href="https://ieeexplore.ieee.org/document/9412791">
	            <papertitle>Gabriella: An Online System for Real-Time Activity Detection in Untrimmed Security Videos</papertitle>
	        </a>
	        <br>
	        <a href="https://nayeemrizve.github.io/">Mamshad Nayeem Rizve</a>, 
	        Ugur Demir, 
	        <a href="https://ptirupat.github.io/">Praveen Tirupattur</a>, 
	        Aayush Jung Rana, 
	        Kevin Duarte, 
	        <strong>Ishan R Dave</strong>, 
	        <a href="https://www.crcv.ucf.edu/person/rawat/">Yogesh S Rawat</a>, 
	        <a href="https://www.crcv.ucf.edu/person/mubarak-shah/">Mubarak Shah</a>.
	        <br>
	        <em>25th International Conference on Pattern Recognition (<strong>ICPR</strong>)</em>, 2021 <em><strong><font color="red">(Best Paper Award)</font></strong></em>
	        <br>
		<div class="paper-links">
		  <a href="https://arxiv.org/pdf/2004.11475" class="paper-link-btn">PDF</a>
		  <a class="toggle-description" id="toggle-desc12" onclick="toggleDescription('desc12')">[show abstract]</a>
		</div>
		<div id="desc12" class="paper-description">
	        <p>
	            Gabriella consists of three stages: tubelet extraction, activity classification, and online tubelet merging. Gabriella utilizes a localization network for tubelet extraction, with a novel Patch-Dice loss to handle variations in actor size, and a Tubelet-Merge Action-Split (TMAS) algorithm to detect activities efficiently and robustly.
	        </p>
		</div>    
	    </td>
	</tr>

	<table id="patents" style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Patents</heading>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
	
	<tr onmouseout="mira_stop()" onmouseover="mira_start()">
	    <td style="padding:20px;width:25%;vertical-align:top">
		<div class="one">
		    <img src='images/avr_eccv2024.png' width="180">
		</div>
	    </td>
	    <td style="padding:20px;width:75%;vertical-align:top">
		<a href="https://patents.google.com/patent/US20250342699A1/en">
		    <papertitle>Identifying and aligning video clips from large-scale video datasets System</papertitle>
		</a>
		<br>
		<a href="https://sjenni.github.io/">Simon Jenni</a>, <strong>Ishan Rajendrakumar Dave</strong>, <a href="https://fabiancaba.com/">Fabian Caba</a>
		<br>
		<em> US Patent US20250342699A1. (Status: Filed) </em>, 2025
	    </td>
	</tr>

		<!-- SPAct Patent -->

	<tr onmouseout="mira_stop()" onmouseover="mira_start()">
	    <td style="padding:20px;width:25%;vertical-align:top">
		<div class="one">
		    <img src='images/spact_patent2023.jpg' width="180">
		</div>
	    </td>
	    <td style="padding:20px;width:75%;vertical-align:top">
		<a href="https://patentimages.storage.googleapis.com/58/6c/b0/8c47ac8fac2691/US12142053.pdf"">
		    <papertitle>Self-Supervised Privacy Preservation Action Recognition System</papertitle>
		</a>
		<br>
		<strong>Ishan Rajendrakumar Dave</strong>, <a href="https://www.crcv.ucf.edu/chenchen/">Chen Chen</a>, <a href="https://www.crcv.ucf.edu/person/mubarak-shah/">Mubarak Shah</a>
		<br>
		<em> US Patent US12142053B2. (Status: <strong>Granted</strong>) </em>, 2024
	    </td>
	</tr>


	<table id="recognition" width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Recognition</heading>
            </td>
          </tr>
        </tbody></table>

        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
	  <td style="padding:20px;width:25%;text-align:center;vertical-align:middle"><img src="images/trophy.png" width="50%" alt="Trophy Image"></td>
				
          <td width="75%" valign="center">
		<!-- Outstanding Reviewer 2024 -->
		    <div>
		        <a href="https://twitter.com/CVPR/status/1793616950314369239/photo/1">
		            <strong>Outstanding Reviewer,</strong> 2024 - 
		            Ranked in the top 1% for review quality among 10,000 reviewers (<strong>CVPR</strong>)
		        </a>
		    </div>
		    <br>
		  
    		<!-- 1st Prize 2023 -->
		    <div>
		        <a href="https://www.crcv.ucf.edu/2023/09/21/congratulations-to-crcv-ph-d-students-jyoti-kini-ishan-dave-and-undergraduate-student-sarah-fleischer/">
		            <strong>1<sup>st</sup> place,</strong> 2023 - 
		            Multi-modal Action Recognition challenge (<strong>ICIAP</strong>)
		        </a>
		    </div>
		    <br>
		
		    <!-- 2nd Prize 2022 -->
		    <div>
<!-- 		        <a href="https://activity-net.org/challenges/2022/challenge.html"> -->
		            <strong>2<sup>nd</sup> place,</strong> 2022 - 
		            ActivityNet ActEV Challenge (<strong>CVPR</strong>)
<!-- 		        </a> -->
		    </div>
		    <br>
		
		    <!-- 2nd Prize 2021 -->
		    <div>
		        <a href="https://www-nlpir.nist.gov/projects/tvpubs/tv21.slides/tv21.actev.slides.pdf">
		            <strong>2<sup>nd</sup> place,</strong> 2021 - 
		            NIST TRECVID ActEV: Activities in Extended Video
		        </a>
		    </div>
		    <br>
		
		    <!-- 1st Prize & Jury Prize 2021 -->
		    <div>
		        <a href="https://vipriors.github.io/2021/challenges/#action-recognition">
		            <strong>1<sup>st</sup> place & Jury Prize,</strong> 2021 - 
		            VI-Priors Action Recognition Challenge (<strong>ICCV</strong>)
		        </a>
		    </div>
		    <br>
		
		    <!-- 1st Prize 2021 PMiss@0.02tfa -->
		    <div>
<!-- 		        <a href="#"> -->
		            <strong>1<sup>st</sup> place,</strong> 2021 - 
		            PMiss@0.02tfa, ActivityNet ActEV SDL (<strong>CVPR</strong>)
<!-- 		        </a> -->
		    </div>
		    <br>
		
		    <!-- 1st Prize 2020 VI-Priors -->
		    <div>
		        <a href="https://vipriors.github.io/2020/challenges/#action-recognition">
		            <strong>1<sup>st</sup> place,</strong> 2020 - 
		            VI-Priors Action Recognition Challenge (<strong>ECCV</strong>)
		        </a>
		    </div>
		    <br>
		
		    <!-- 1st Prize 2020 PMiss and nAUDC -->
		    <div>
<!-- 		        <a href="#"> -->
		            <strong>1<sup>st</sup> place,</strong> 2020 - 
		            PMiss and nAUDC, ActivityNet ActEV SDL (<strong>CVPR</strong>)
<!-- 		        </a> -->
		    </div>
		    <br>
		
		    <!-- 2nd Prize 2020 TRECVID -->
		    <div>
<!-- 		        <a href="#"> -->
		            <strong>2<sup>nd</sup> place,</strong> 2020 - 
		            TRECVID ActEV: Activities in Extended Video
<!-- 		        </a> -->
		    </div>
		    <br>
		
		    <!-- ORCGS Doctoral Fellowship 2019-2020 -->
		    <div>
<!-- 		        <a href="#"> -->
		            <strong>ORCGS Doctoral Fellowship,</strong> 2019-2020
<!-- 		        </a> -->
		    </div>
		    <br>
		
		    <!-- Top 0.5% 2013 -->
		    <div>
		        <a href="#">
		            <strong>Top 0.5%,</strong> 2013 - 
		            Joint Engineering Entrance-Mains exam, India
		        </a>
		    </div>
		</td>
		
	<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Professional Reviewing experience</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
					
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/cvf.jpg"></td>
            <td width="75%" valign="center">
		<a href="https://www.computer.org/csdl/proceedings-article/cvpr/2024/530000z352/20hQ1juBJfi">Reviewer, CVPR 2025, 2024, 2023, 2022</a><br>
		<a href="https://eccv2024.ecva.net/">Reviewer, ECCV 2024</a><br>
		<a href="https://sites.google.com/view/pfatcvbmvc24/home">Program Committee, AAAI 2025</a><br>	
    		<a href="https://iccv2023.thecvf.com/">Reviewer, ICCV 2023</a><br>
		<a href="https://sites.google.com/view/pfatcvbmvc24/home">Technical Committee, BMVC Workshop 2024</a><br>	
		<a href="https://wacv2025.thecvf.com/">Reviewer, WACV 2025, 2024</a><br>	
		<a href="https://www.ieee-ras.org/publications/ra-l">Reviewer, IEEE Robotics and Automation Letters</a><br>	
		<a href="https://2024.ieee-icra.org/">Reviewer, ICRA 2024</a><br>	
		<a href="https://iros2024-abudhabi.org/">Reviewer, IROS 2024</a><br>	
	      	<a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=83">Reviewer, IEEE Transaction on Image Processing</a><br>
	        <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34">Reviewer, IEEE Transaction on Pattern Analysis and Machine Intelligence</a><br>
		<a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=6046">Reviewer, IEEE Transactions on Multimedia</a><br>
	        <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=76">Reviewer, IEEE Transactions on Circuits and Systems for Video Technology</a><br>
		<a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=5962385">Reviewer, IEEE Transactions on Neural Networks and Learning Systems</a><br>    
	        <a href="https://www.sciencedirect.com/journal/computer-vision-and-image-understanding">Reviewer, Computer Vision and Image Understanding</a><br>
		<a href="https://www.sciencedirect.com/journal/pattern-recognition">Reviewer, Pattern Recognition</a><br>
		<a href="https://www.sciencedirect.com/journal/expert-systems-with-applications">Reviewer, Expert Systems with Applications</a><br>
		<a href="https://www.sciencedirect.com/journal/image-and-vision-computing">Reviewer, Image and Vision Computing</a><br>
		<a href="https://link.springer.com/journal/11554">Reviewer, Journal of Real-Time Image Processing</a><br>
		<a href="https://link.springer.com/journal/11042">Reviewer, Multimedia Tools and Applications</a><br>

              <br>
              <br>
             
		    
            </td>
          </tr>	

		
	<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Mentor in NSF-REU</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
					
          <tr>
            <td style="padding:20px;width:25%;text-align:center;vertical-align:middle"><img src="images/NSF_svg.png" width="50%" alt="NSF Image"></td>
            <td width="75%" valign="center">
		<a href="https://www.crcv.ucf.edu/nsf-projects/reu/reu-2022/">Kevin Chung, REU 2022</a>
	        <br>

		<a href="https://www.crcv.ucf.edu/nsf-projects/reu/reu-2021/">Ethan Thomas, REU 2021</a>
		<br>
		<a href="https://www.crcv.ucf.edu/nsf-projects/reu/reu-2020/">Kali Carter, REU 2020</a>

              <br>
              <br>
		    
            </td>
          </tr>

		
					
					
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Feel free to steal this website's <a href="https://github.com/jonbarron/jonbarron_website">source code</a>. <strong>Do not</strong> scrape the HTML from this page itself, as it includes analytics tags that you do not want on your own website &mdash; use the github code instead. Also, consider using <a href="https://leonidk.com/">Leonid Keselman</a>'s <a href="https://github.com/leonidk/new_website">Jekyll fork</a> of this page.
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
