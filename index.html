<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Ishan Dave</title>
  
  <meta name="author" content="Ishan Dave">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.0.7/css/all.css">

	<!-- <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>"> -->
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Ishan Dave</name>
              </p>
              <p style="font-size:15px">I am a fifth-year Ph.D. student in the Center for Research in Computer Vision (CRCV), University of Central Florida (UCF), advised by <a href="https://www.crcv.ucf.edu/person/mubarak-shah/" style="font-size:15px">Prof. Mubarak Shah</a>. </a>.
              </p>
              <p style="text-align:center">
                <a href="mailto:ishandave95@gmail.com">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?hl=en&user=fWu6sFgAAAAJ&view_op=list_works&sortby=pubdate">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/daveishan">Github</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/ishan-dave-crcv/">LinkedIn</a> &nbsp/&nbsp
                <a href="data/CVpublic_21Aug2024_Dave.pdf">CV (it's a public version, for a detailed version feel free to ask!)</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/ishan_pic-modified.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/ishan_pic-modified.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

	<table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <h2>Updates <strong><font color="red">Looking for fulltime jobs!</font></strong> </h2>
                  <p>
                    	  <span style="font-family: 'Courier New', Courier, monospace; color: #696969;">&nbsp;<i class="fa fa-share-alt" style="font-size:12px"></i> August 2024:</span> SPAct Patent Approved! my first patent as the primary inventor üí•<br>
			  <span style="font-family: 'Courier New', Courier, monospace; color: #696969;">&nbsp;<i class="fa fa-share-alt" style="font-size:12px"></i> July 2024:</span> 2 First author papers accepted at <strong>ECCV 2024</strong>: <font color="red">Oral presentation! (Top 3% of accepted papers)</font>üí•üí•<br>
			  <span style="font-family: 'Courier New', Courier, monospace; color: #696969;">&nbsp;<i class="fa fa-share-alt" style="font-size:12px"></i> June 2024:</span> Selected as Outstanding Reviewer of CVPR 2024! (top 2% among 10,000 reviewers)ü•á<br>
			  <span style="font-family: 'Courier New', Courier, monospace; color: #696969;">&nbsp;<i class="fa fa-share-alt" style="font-size:12px"></i> May 2024:</span> Started internship at Apple, Cupertino, CA<br>
			  <span style="font-family: 'Courier New', Courier, monospace; color: #696969;">&nbsp;<i class="fa fa-share-alt" style="font-size:12px"></i> December 2023:</span> A First author paper "No More Shortcuts" accepted to <strong>AAAI 2024</strong> üí•<br>
			  <span style="font-family: 'Courier New', Courier, monospace; color: #696969;">&nbsp;<i class="fa fa-share-alt" style="font-size:12px"></i> July 2023:</span> A First author paper "Event-TransAct" accepted at IROS 2023 üí•<br>
			  <span style="font-family: 'Courier New', Courier, monospace; color: #696969;">&nbsp;<i class="fa fa-share-alt" style="font-size:12px"></i> July 2023:</span> A paper "TeD-SPAD" accepted at ICCV 2023<br>
			  <span style="font-family: 'Courier New', Courier, monospace; color: #696969;">&nbsp;<i class="fa fa-share-alt" style="font-size:12px"></i> May 2023:</span> Started summer internship at Adobe, San Jose, CA<br>
			  <span style="font-family: 'Courier New', Courier, monospace; color: #696969;">&nbsp;<i class="fa fa-share-alt" style="font-size:12px"></i> March 2023:</span> A First author paper "TimeBalance" accepted to <strong>CVPR 2023</strong> üí•<br>
			  <span style="font-family: 'Courier New', Courier, monospace; color: #696969;">&nbsp;<i class="fa fa-share-alt" style="font-size:12px"></i> Jan 2023:</span> A paper "TransVisDrone" accepted at ICRA 2023<br>
			  <span style="font-family: 'Courier New', Courier, monospace; color: #696969;">&nbsp;<i class="fa fa-share-alt" style="font-size:12px"></i> May 2022:</span> Started summer internship at Adobe, USA (remote- Florida)<br>
			  <span style="font-family: 'Courier New', Courier, monospace; color: #696969;">&nbsp;<i class="fa fa-share-alt" style="font-size:12px"></i> March 2022:</span> A First author paper "TCLR" accepted to CVIU 2022 üí•<br>
			  <span style="font-family: 'Courier New', Courier, monospace; color: #696969;">&nbsp;<i class="fa fa-share-alt" style="font-size:12px"></i> March 2022:</span> A First author paper "SPAct" accepted to <strong>CVPR 2022</strong> üí•<br>
			  <span style="font-family: 'Courier New', Courier, monospace; color: #696969;">&nbsp;<i class="fa fa-share-alt" style="font-size:12px"></i> January 2021:</span> Our Gabriella paper has been awarded the best scientific paper award at ICPR 2020<br>
<!--                     &nbsp <i class="fa fa-share-alt" style="font-size:12px"></i> June 2020: Placed first at ActEV SDL Challenge (ActivityNet workshop at CVPR 2020) ü•á<br>
                    &nbsp <i class="fa fa-share-alt" style="font-size:12px"></i> October 2019: Placed second at the TRECVID leaderboard ü•á<br> -->
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
	
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Work Experience</heading>
            </td>
          </tr>
          </table>
        
	<table width="100%" align="center" border="0" cellpadding="20"></tbody>

            <tr>
              <td style="padding:10px;width:25%;vertical-align:top; text-align:center">
		<img src='images/apple_logo.png' style="width:50%;">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <strong>PhD AI/ML Intern</strong>
                <br> Apple Inc., Cupertino, California, USA. May 2024- Ongoing
	        <p></p>
		<ul style="margin: 0; padding-left: 20px; line-height: 1.2;">
		    <li>Enhanced stable diffusion models for image editing by leveraging vision-language multimodal foundation models.</li>
		    <li>Trained diffusion models on a large-scale, high-resolution dataset of 10M samples.</li>
		    <li>Reproduced and outperformed state-of-the-art image editing methods using a novel approach, achieving superior results.</li>
		</ul>	      
              </td>
		<p></p>
              </td>
            </tr>
          </tr>
          </table>
	
          <table width="100%" align="center" border="0" cellpadding="20"></tbody>

            <tr>
              <td style="padding:10px;width:25%;vertical-align:top">
		<img src='images/Adobe-logo.png' style="width:100%;">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <strong>Research Scientist/ Engineer Intern</strong>
                <br> Adobe Inc., San Jose, California, USA. May 2023- Nov 2023
                <br> Host: <a href="https://sjenni.github.io/">Simon Jenni, </a>
                <a href="https://fabiancaba.com/">Fabian Caba</a>
                <p></p>
                <ul style="margin: 0; padding-left: 20px; line-height: 1.2;">
		    <li>Enhanced the fine-grained capabilities of existing video retrieval methods.</li>
		    <li>Worked on large-scale video galleries with millions of samples.</li>
		    <li>Filed a patent and had a paper accepted at ECCV 2024.</li>
		</ul>	      
              </td>
            </tr>
          </tr>
          </table>
	 <table width="100%" align="center" border="0" cellpadding="20"></tbody>

            <tr>
               <td style="padding:10px;width:25%;vertical-align:top">
		<img src='images/Adobe-logo.png' style="width:100%;">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <strong>Research Scientist Intern</strong>
                <br> Adobe Inc., Remote, USA. May 2022 - Nov 2022
                <br> Host: <a href="https://sjenni.github.io/">Simon Jenni</a>
                <p></p>
                <ul style="margin: 0; padding-left: 20px; line-height: 1.2;">
		    <li>Developed a novel self-supervised video representation framework by reformulating temporal self-supervision as frame-level recognition tasks and introducing an effective augmentation strategy to mitigate shortcuts.</li>
		    <li>Achieved state-of-the-art performance on 10 video understanding benchmarks across linear classification (Kinetics400, HVU, SSv2, Charades), video retrieval (UCF101, HMDB51), and temporal correspondence (CASIA-B).</li>
		    <li>Published a paper at AAAI 2024.</li>
		</ul>	      
              </td>
            </tr>
          </tr>
          </table>

         

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                I have a broad interest in computer vision and machine learning. My current research mainly focuses on video representation learning with limited labels (self/semi-supervised learning), action recognition, and privacy preservation in video understanding tasks. My recent research also includes enhancing the fine-grained video understanding of the large foundational models and improving multi-modal generative AI. I have also worked on various robotics-related vision tasks like event-camera-based action recognition and drone-to-drone detections from videos. 
                <br> Below is a selected list of my works (in <strong>chronological order</strong>), representative papers are <span style="background-color: #ffffd0;">highlighted</span>.

              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

	<tr onmouseout="mira_stop()" onmouseover="mira_start()">
	    <td style="padding:20px;width:25%;vertical-align:middle">
		<div class="one">
		    <img src='images/avr_eccv2024.png' width="180">
		</div>
	    </td>
	    <td style="padding:20px;width:75%;vertical-align:middle">
		<a href="https://www.crcv.ucf.edu/wp-content/uploads/2018/11/avr_eccv24_dave.pdf">
		    <papertitle>Sync from the Sea: Retrieving Alignable Videos from Large-Scale Datasets</papertitle>
		</a>
		<br>
		<strong>Ishan Rajendrakumar Dave</strong>, 
		Fabian Caba, 
		Mubarak Shah, 
		Simon Jenni.
		<br>
		<em>The 18th European Conference on Computer Vision (<strong>ECCV</strong>) </em>, 2024
		<br>
	       <em><strong><font color="red">Oral presentation! (Top 3% of accepted papers)</font></strong></em>
		<br>
		<a href="https://daveishan.github.io/avr-webpage/">project page</a> 
		<p></p>
		<p>
			Temporal video alignment synchronizes key events like object interactions or action phase transitions in two videos, benefiting video editing, processing, and understanding tasks. Existing methods assume a given video pair, limiting applicability. We redefine this as a search problem, introducing Alignable Video Retrieval (AVR), which identifies and synchronizes well-alignable videos from a large collection. Key contributions include DRAQ, a video alignability indicator, and a generalizable frame-level video feature design.

		</p>
	     </td>
	</tr>

	<tr onmouseout="mira_stop()" onmouseover="mira_start()" bgcolor="#ffffd0">	
		<td style="padding:20px;width:25%;vertical-align:middle">
			        <div class="one">
			            <img src='images/finepsuedo_eccv2024.png' width="180">
			        </div>
			    </td>
			    <td style="padding:20px;width:75%;vertical-align:middle">
		        <a href="https://www.crcv.ucf.edu/wp-content/uploads/2018/11/finepsuedo_eccv24_dave.pdf">
		            <papertitle>FinePseudo: Improving Pseudo-Labelling through Temporal-Alignablity for Semi-Supervised Fine-Grained Action Recognition</papertitle>
		        </a>
		        <br>
		        <strong>Ishan Rajendrakumar Dave</strong>, 
		        Mamshad Nayeem Rizve, 
		        Mubarak Shah.
		        <br>
		        <em>The 18th European Conference on Computer Vision (<strong>ECCV</strong>) </em>, 2024
		        <br>
			<a href="https://daveishan.github.io/finepsuedo-webpage/">project page</a> 
		   	<p></p>
			<p>
				We introduce Alignability-Verification-based Metric learning for semi-supervised fine-grained action recognition. Using dynamic time warping (DTW) for action-phase-aware comparison, our learnable alignability score refines pseudo-labels of the video encoder. Our framework, FinePseudo, outperforms prior methods on fine-grained action recognition datasets. Additionally, it demonstrates robustness in handling novel unlabeled classes in open-world setups.
			</p>
		    	</td>
	</tr>

		
<!-- 	<tr onmouseout="mira_stop()" onmouseover="mira_start()" bgcolor="#ffffd0"> -->
		
	    <td style="padding:20px;width:25%;vertical-align:middle">
	        <div class="one">
	            <img src='images/codamal_icip2024.png' width="180">
	        </div>
	    </td>
	    <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/pdf/2312.13008.pdf">
            <papertitle>CodaMal: Contrastive Domain Adaptation for Malaria Detection in Low-Cost Microscopes</papertitle>
        </a>
        <br>
        <strong>Ishan Rajendrakumar Dave</strong>, 
	Tristan de Blegiers, 
	Chen Chen,
        Mubarak Shah.
        <br>
        <em>31st IEEE International Conference on Image Processing (<strong>ICIP</strong>) </em>, 2024
        <br>
	<a href="https://daveishan.github.io/codamal-webpage/">project page</a> &nbsp/&nbsp <a href="https://github.com/DAVEISHAN/CodaMal">code</a>
   	<p></p>
	<p>
		We propose a Domain Adaptive Contrastive objective to bridge the gap between High and Low Cost Microscopes. On the publicly available large-scale M5 dataset, our proposed method shows a significant improvement of 16% over the state-of-the-art methods in terms of the mean average precision metric (mAP), provides a 21√ó speed-up during inference, and requires only half as many learnable parameters as the prior methods.

	</p>
    	</td>
<!-- 	</tr> -->



		
	<tr onmouseout="mira_stop()" onmouseover="mira_start()" bgcolor="#ffffd0">
	    <td style="padding:20px;width:25%;vertical-align:middle">
	        <div class="one">
	            <img src='images/nms_aaai2024.png' width="180">
	        </div>
	    </td>
	    <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/pdf/2312.13008.pdf">
            <papertitle>No More Shortcuts: Realizing the Potential of Temporal Self-Supervision</papertitle>
        </a>
        <br>
        <strong>Ishan Rajendrakumar Dave</strong>, 
        Simon Jenni,  
        Mubarak Shah.
        <br>
        <em>AAAI Conference on Artificial Intelligence, Main Technical Track (<strong>AAAI</strong>) </em>, 2024
        <br>
	<a href="https://daveishan.github.io/nms-webpage/">project page</a>
   	<p></p>
	<p>
		We demonstrate experimentally that our more challenging frame-level task formulations and the removal of shortcuts drastically improve the quality of features learned through temporal self-supervision. Our extensive experiments show state-of-the-art performance across 10 video understanding datasets, illustrating the generalization ability and robustness of our learned video representations.
	</p>
    	</td>
	</tr>
		
		
	<tr onmouseout="mira_stop()" onmouseover="mira_start()">
	    <td style="padding:20px;width:25%;vertical-align:middle">
		<div class="one">
		    <img src='images/tedspad_iccv2023.png' width="180">
		</div>
	    </td>
	    <td style="padding:20px;width:75%;vertical-align:middle">
		<a href="LINK_TO_PAPER">
		    <papertitle>TeD-SPAD: Temporal Distinctiveness for Self-supervised Privacy-preservation for Video Anomaly Detection</papertitle>
		</a>
		<br>
		Joseph Fioresi, 
		<strong>Ishan Rajendrakumar Dave</strong>, 
		Mubarak Shah.
		<br>
		<em>Proceedings of the IEEE/CVF International Conference on Computer Vision (<strong>ICCV</strong>)</em>, 2023
		<br>
		<a href="https://arxiv.org/pdf/2308.11072.pdf">paper</a> &nbsp/&nbsp <a href="https://github.com/UCF-CRCV/TeD-SPAD">code</a> &nbsp/&nbsp <a href="https://joefioresi718.github.io/TeD-SPAD_webpage/">project page</a>
		    <p></p>
			<p>
			    We propose TeD-SPAD, a privacy-aware video anomaly detection framework that destroys visual private information in a self-supervised manner. In particular, we propose the use of a temporally-distinct triplet loss to promote temporally discriminative features, which complements current weakly-supervised VAD methods.
			</p>
	    </td>
	</tr>

	<tr onmouseout="mira_stop()" onmouseover="mira_start()">
	    <td style="padding:20px;width:25%;vertical-align:middle">
	        <div class="one">
	            <img src='images/eventtransact_iros2023.png' width="180">
	        </div>
	    </td>
	    <td style="padding:20px;width:75%;vertical-align:middle">
	        <a href="LINK_TO_PAPER">
	            <papertitle>EventTransAct: A Video Transformer-based Framework for Event-camera Based Action Recognition</papertitle>
	        </a>
	        <br>
	        Tristan de Blegiers*, 
	        <strong>Ishan Rajendrakumar Dave*</strong>, 
	        Adeel Yousaf, 
	        Mubarak Shah.
		<br>
		*= equal contribution
	        <br>
	        <em>IEEE/RSJ International Conference on Intelligent Robots and Systems (<strong>IROS</strong>)</em>, 2023
	        <br>
	        <a href="LINK_TO_PAPER">paper</a> &nbsp/&nbsp <a href="https://github.com/tristandb8/EventTransAct">code</a> &nbsp/&nbsp <a href="https://joefioresi718.github.io/TeD-SPAD_webpage/">project page</a>
	    	<p></p>
		<p>
		   We propose a video transformer-based framework for event-camera based action recognition, which leverages event-contrastive loss and augmentations to adapt the network to event data. Our method achieved state-of-the-art results on N-EPIC Kitchens dataset and competitive results on the standard DVS Gesture recognition dataset, while requiring less computation time compared to competitive prior approaches. 
		</p>
	    </td>
	</tr>

		
		
          <tr onmouseout="mira_stop()" onmouseover="mira_start()" bgcolor="#ffffd0">
	    <td style="padding:20px;width:25%;vertical-align:middle">
	        <div class="one">
	            <img src='images/timebalance_cvpr2023.png' width="180">
	        </div>
	    </td>
	    <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Dave_TimeBalance_Temporally-Invariant_and_Temporally-Distinctive_Video_Representations_for_Semi-Supervised_Action_Recognition_CVPR_2023_paper.pdf">
            <papertitle>TimeBalance: Temporally-Invariant and Temporally-Distinctive Video Representations for Semi-Supervised Action Recognition</papertitle>
        </a>
        <br>
        <strong>Ishan Rajendrakumar Dave</strong>, 
        Mamshad Nayeem Rizve, 
        Chen Chen, 
        Mubarak Shah.
        <br>
        <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2023
        <br>
        <a href="https://arxiv.org/pdf/2303.16268.pdf">paper</a> &nbsp/&nbsp <a href="https://github.com/DAVEISHAN/TimeBalance">code</a> &nbsp/&nbsp <a href="https://daveishan.github.io/timebalance_webpage/">project page</a>
   	<p></p>
	<p>
		We propose a student-teacher semi-supervised learning framework, where we distill knowledge from a temporally-invariant and temporally-distinctive teacher. Depending on the nature of the unlabeled video, we dynamically combine the knowledge of these two teachers based on a novel temporal similarity-based reweighting scheme. State-of-the-art results on Kinetics400, UCF101, HMDB51.

	</p>
    	</td>
	</tr>

	<tr onmouseout="mira_stop()" onmouseover="mira_start()">
	    <td style="padding:20px;width:25%;vertical-align:middle">
	        <div class="one">
	            <img src='images/transvisdrone_icra23.png' width="180">
	        </div>
	    </td>
	    <td style="padding:20px;width:75%;vertical-align:middle">
	        <a href="https://ieeexplore.ieee.org/document/10161433">
	            <papertitle>Transvisdrone: Spatio-temporal Transformer for Vision-based Drone-to-drone Detection in Aerial Videos</papertitle>
	        </a>
	        <br>
	        Tushar Sangam, 
	        <strong>Ishan Rajendrakumar Dave</strong>,
	        Waqas Sultani, 
	        Mubarak Shah.
	        <br>
	        <em>2023 IEEE International Conference on Robotics and Automation (<strong>ICRA</strong>)</em>, 2023
	        <br>
	        <a href="https://arxiv.org/pdf/2210.08423.pdf">paper</a> &nbsp/&nbsp <a href="https://github.com/tusharsangam/TransVisDrone">code</a> &nbsp/&nbsp <a href="https://tusharsangam.github.io/TransVisDrone-project-page/">project page</a>
			<p></p>
			<p>
			    We propose a simple yet effective framework, TransVisDrone, that provides an end-to-end solution with higher computational efficiency. We utilize CSPDarkNet-53 network to learn object-related spatial features and VideoSwin model to improve drone detection in challenging scenarios by learning spatio-temporal dependencies of drone motion.
			</p>
	    </td>
	</tr>

	

	<tr onmouseout="mira_stop()" onmouseover="mira_start()" bgcolor="#ffffd0">
	    <td style="padding:20px;width:25%;vertical-align:middle">
	        <div class="one">
	            <img src='images/spact_cvpr2022.png' width="180">
	        </div>
	    </td>
	    <td style="padding:20px;width:75%;vertical-align:middle">
	        <a href="https://openaccess.thecvf.com/content/CVPR2022/html/Dave_SPAct_Self-Supervised_Privacy_Preservation_for_Action_Recognition_CVPR_2022_paper.html">
	            <papertitle>SPAct: Self-supervised Privacy Preservation for Action Recognition</papertitle>
	        </a>
	        <br>
	        <strong>Ishan Rajendrakumar Dave</strong>, 
	        Chen Chen, 
	        Mubarak Shah.
	        <br>
	        <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2022
	        <br>
	        <a href="https://arxiv.org/pdf/2203.15205.pdf">paper</a> &nbsp/&nbsp <a href="https://github.com/DAVEISHAN/SPAct">code</a>
		    <p></p>
			<p>
			    For the first time, we present a novel training framework that removes privacy information from input video in a self-supervised manner without requiring privacy labels. We train our framework using a minimax optimization strategy to minimize the action recognition cost function and maximize the privacy cost function through a contrastive self-supervised loss. 
			</p>
	    </td>
	</tr>

	

	
		
	<tr onmouseout="mira_stop()" onmouseover="mira_start()" bgcolor="#ffffd0">
	    <td style="padding:5px;width:25%;vertical-align:middle">
	        <div class="one">
	            <img src='images/tclr_cviu2022.png' width="180">
	        </div>
	        <script type="text/javascript">
	            function mira_start() {
	                document.getElementById('mira_image').style.opacity = "1";
	            }
	
	            function mira_stop() {
	                document.getElementById('mira_image').style.opacity = "0";
	            }
	            mira_stop()
	        </script>
	    </td>
	    <td style="padding:20px;width:75%;vertical-align:middle">
	        <a href="https://www.sciencedirect.com/science/article/pii/S1077314222000376">
	            <papertitle>TCLR: Temporal Contrastive Learning for Video Representation</papertitle>
	        </a>
	        <br>
	        <strong>Ishan Dave</strong>,
	        Rohit Gupta, 
	        Mamshad Nayeem Rizve, 
	        Mubarak Shah.
	        <br>
	        <em>Computer Vision and Image Understanding (<strong>CVIU</strong>)</em>, 2022
	        <br>
	       <em><strong><font color="red">(100+ citations, Among the top-10 most downloaded papers in CVIU)</font></strong></em>
               <br>
	        <a href="https://arxiv.org/pdf/2101.07974.pdf">paper</a> &nbsp/&nbsp
	        <a href="https://github.com/DAVEISHAN/TCLR">code</a>
	        <p></p>
	        <p>
	            We propose a new temporal contrastive learning framework for self-supervised video representation learning, consisting of two novel losses that aim to increase the temporal diversity of learned features. The framework achieves state-of-the-art results on various downstream video understanding tasks, including significant improvement in fine-grained action classification for visually similar classes.
	        </p>
	    </td>
	</tr>

	<tr onmouseout="mira_stop()" onmouseover="mira_start()">
	    <td style="padding:20px;width:25%;vertical-align:middle">
	        <div class="one">
	            <img src='images/gabriellav2_wacv22.png' width="180">
	        </div>
	    </td>
	    <td style="padding:20px;width:75%;vertical-align:middle">
	        <a href="https://openaccess.thecvf.com/content/WACV2022W/HADCV/papers/Dave_GabriellaV2_Towards_Better_Generalization_in_Surveillance_Videos_for_Action_Detection_WACVW_2022_paper.pdf">
	            <papertitle>Gabriellav2: Towards Better Generalization in Surveillance Videos for Action Detection</papertitle>
	        </a>
	        <br>
	        <strong>Ishan Dave</strong>, 
	        Zacchaeus Scheffer, 
	        Akash Kumar, 
	        Sarah Shiraz, 
	        Yogesh Singh Rawat, 
	        Mubarak Shah.
	        <br>
	        <em>Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (<strong>WACV</strong>)</em>, 2022
	        <br>
	        <a href="https://openaccess.thecvf.com/content/WACV2022W/HADCV/papers/Dave_GabriellaV2_Towards_Better_Generalization_in_Surveillance_Videos_for_Action_Detection_WACVW_2022_paper.pdf">paper</a>
	        <p></p>
	        <p>
	            We propose a realtime, online, action detection system which can generalize robustly on any unknown facility surveillance videos. We tackle the
challenging nature of action classification problem in various aspects like handling the class-imbalance training using PLM method and learning multi-label action correlations using LSEP loss. In order to improve the computational efficiency of the system, we utilize knowledge distillation.
	        </p>
	    </td>
	</tr>	
	<tr onmouseout="mira_stop()" onmouseover="mira_start()">
	    <td style="padding:20px;width:25%;vertical-align:middle">
	        <div class="one">
	            <img src='images/gabriella_icpr2020.png' width="180">
	        </div>
	    </td>
	    <td style="padding:20px;width:75%;vertical-align:middle">
	        <a href="https://ieeexplore.ieee.org/document/9412791">
	            <papertitle>Gabriella: An Online System for Real-Time Activity Detection in Untrimmed Security Videos</papertitle>
	        </a>
	        <br>
	        Mamshad Nayeem Rizve, 
	        Ugur Demir, 
	        Praveen Tirupattur, 
	        Aayush Jung Rana, 
	        Kevin Duarte, 
	        <strong>Ishan R Dave</strong>, 
	        Yogesh S Rawat, 
	        Mubarak Shah.
	        <br>
	        <em>25th International Conference on Pattern Recognition (<strong>ICPR</strong>)</em>, 2021 <em><strong><font color="red">(Best Paper Award)</font></strong></em>
	        <br>
	        <a href="https://arxiv.org/abs/2004.11475">paper</a> 
		<p></p>
	        <p>
	            Gabriella consists of three stages: tubelet extraction, activity classification, and online tubelet merging. Gabriella utilizes a localization network for tubelet extraction, with a novel Patch-Dice loss to handle variations in actor size, and a Tubelet-Merge Action-Split (TMAS) algorithm to detect activities efficiently and robustly.
	        </p>    
	    </td>
	</tr>

	<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Patent</heading>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

	<tr onmouseout="mira_stop()" onmouseover="mira_start()">
	    <td style="padding:20px;width:25%;vertical-align:middle">
		<div class="one">
		    <img src='images/spact_patent2023.jpg' width="180">
		</div>
	    </td>
	    <td style="padding:20px;width:75%;vertical-align:middle">
		<a href="LINK_TO_PAPER">
		    <papertitle>Self-Supervised Privacy Preservation Action Recognition System</papertitle>
		</a>
		<br>
		<strong>Ishan Rajendrakumar Dave</strong>, 
		Chen Chen,
		Mubarak Shah, 	
		<br>
		<em> The University of Central Florida. Invention Track Code: 2023-019. (Status: Approved) </em>, 2023
		<br>
		<a href="https://ucf.flintbox.com/technologies/781d7a2d-1f89-41db-911c-5d4ecf26af20">Tech Sheet</a>
		    
	    </td>
	</tr>


	<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Awards / Recognitions</heading>
            </td>
          </tr>
        </tbody></table>

        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
	  <td style="padding:20px;width:25%;text-align:center;vertical-align:middle"><img src="images/trophy.png" width="50%" alt="Trophy Image"></td>
				
          <td width="75%" valign="center">
		<!-- Outstanding Reviewer 2024 -->
		    <div>
		        <a href="https://twitter.com/CVPR/status/1793616950314369239/photo/1">
		            <strong>Outstanding Reviewer,</strong> 2024 - 
		            Ranked in the top 1% for review quality among 10,000 reviewers (<strong>CVPR</strong>)
		        </a>
		    </div>
		    <br>
		  
    		<!-- 1st Prize 2023 -->
		    <div>
		        <a href="https://www.crcv.ucf.edu/2023/09/21/congratulations-to-crcv-ph-d-students-jyoti-kini-ishan-dave-and-undergraduate-student-sarah-fleischer/">
		            <strong>1<sup>st</sup> place,</strong> 2023 - 
		            Multi-modal Action Recognition challenge (<strong>ICIAP</strong>)
		        </a>
		    </div>
		    <br>
		
		    <!-- 2nd Prize 2022 -->
		    <div>
<!-- 		        <a href="https://activity-net.org/challenges/2022/challenge.html"> -->
		            <strong>2<sup>nd</sup> place,</strong> 2022 - 
		            ActivityNet ActEV Challenge (<strong>CVPR</strong>)
<!-- 		        </a> -->
		    </div>
		    <br>
		
		    <!-- 2nd Prize 2021 -->
		    <div>
		        <a href="https://www-nlpir.nist.gov/projects/tvpubs/tv21.slides/tv21.actev.slides.pdf">
		            <strong>2<sup>nd</sup> place,</strong> 2021 - 
		            NIST TRECVID ActEV: Activities in Extended Video
		        </a>
		    </div>
		    <br>
		
		    <!-- 1st Prize & Jury Prize 2021 -->
		    <div>
		        <a href="https://vipriors.github.io/2021/challenges/#action-recognition">
		            <strong>1<sup>st</sup> place & Jury Prize,</strong> 2021 - 
		            VI-Priors Action Recognition Challenge (<strong>ICCV</strong>)
		        </a>
		    </div>
		    <br>
		
		    <!-- 1st Prize 2021 PMiss@0.02tfa -->
		    <div>
<!-- 		        <a href="#"> -->
		            <strong>1<sup>st</sup> place,</strong> 2021 - 
		            PMiss@0.02tfa, ActivityNet ActEV SDL (<strong>CVPR</strong>)
<!-- 		        </a> -->
		    </div>
		    <br>
		
		    <!-- 1st Prize 2020 VI-Priors -->
		    <div>
		        <a href="https://vipriors.github.io/2020/challenges/#action-recognition">
		            <strong>1<sup>st</sup> place,</strong> 2020 - 
		            VI-Priors Action Recognition Challenge (<strong>ECCV</strong>)
		        </a>
		    </div>
		    <br>
		
		    <!-- 1st Prize 2020 PMiss and nAUDC -->
		    <div>
<!-- 		        <a href="#"> -->
		            <strong>1<sup>st</sup> place,</strong> 2020 - 
		            PMiss and nAUDC, ActivityNet ActEV SDL (<strong>CVPR</strong>)
<!-- 		        </a> -->
		    </div>
		    <br>
		
		    <!-- 2nd Prize 2020 TRECVID -->
		    <div>
<!-- 		        <a href="#"> -->
		            <strong>2<sup>nd</sup> place,</strong> 2020 - 
		            TRECVID ActEV: Activities in Extended Video
<!-- 		        </a> -->
		    </div>
		    <br>
		
		    <!-- ORCGS Doctoral Fellowship 2019-2020 -->
		    <div>
<!-- 		        <a href="#"> -->
		            <strong>ORCGS Doctoral Fellowship,</strong> 2019-2020
<!-- 		        </a> -->
		    </div>
		    <br>
		
		    <!-- Top 0.5% 2013 -->
		    <div>
		        <a href="#">
		            <strong>Top 0.5%,</strong> 2013 - 
		            Joint Engineering Entrance-Mains exam, India
		        </a>
		    </div>
		</td>
		
	<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Professional Reviewing experience</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
					
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/cvf.jpg"></td>
            <td width="75%" valign="center">
		<a href="https://sites.google.com/view/pfatcvbmvc24/home">Technical Committee, BMVC Workshop 2024</a><br>	
		<a href="https://eccv2024.ecva.net/">Reviewer, ECCV 2024</a><br>	
		<a href="https://cvpr2022.thecvf.com/area-chairs">Reviewer, CVPR 2024, 2023, 2022</a><br>
		<a href="https://iccv2023.thecvf.com/">Reviewer, ICCV 2023</a><br>
		<a href="https://eccv2024.ecva.net/">Reviewer, WACV 2025, 2024</a><br>	
		<a href="https://2024.ieee-icra.org/">Reviewer, ICRA 2024</a><br>	
		<a href="https://iros2024-abudhabi.org/">Reviewer, IROS 2024</a><br>	
	      	<a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=83">Reviewer, IEEE Transaction on Image Processing</a><br>
	        <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34">Reviewer, IEEE Transaction on Pattern Analysis and Machine Intelligence</a><br>
		<a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=6046">Reviewer, IEEE Transactions on Multimedia</a><br>
	        <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=76">Reviewer, IEEE Transactions on Circuits and Systems for Video Technology</a><br>
		<a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=5962385">Reviewer, IEEE Transactions on Neural Networks and Learning Systems</a><br>    
	        <a href="https://www.sciencedirect.com/journal/computer-vision-and-image-understanding">Reviewer, Computer Vision and Image Understanding</a><br>
		<a href="https://www.sciencedirect.com/journal/pattern-recognition">Reviewer, Pattern Recognition</a><br>
		<a href="https://www.sciencedirect.com/journal/expert-systems-with-applications">Reviewer, Expert Systems with Applications</a><br>
		<a href="https://www.sciencedirect.com/journal/image-and-vision-computing">Reviewer, Image and Vision Computing</a><br>
		<a href="https://link.springer.com/journal/11554">Reviewer, Journal of Real-Time Image Processing</a><br>
		<a href="https://link.springer.com/journal/11042">Reviewer, Multimedia Tools and Applications</a><br>

              <br>
              <br>
             
		    
            </td>
          </tr>	

		
	<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Mentor in NSF-REU</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
					
          <tr>
            <td style="padding:20px;width:25%;text-align:center;vertical-align:middle"><img src="images/NSF_svg.png" width="50%" alt="NSF Image"></td>
            <td width="75%" valign="center">
		<a href="https://www.crcv.ucf.edu/nsf-projects/reu/reu-2022/">Kevin Chung, REU 2022</a>
	        <br>

		<a href="https://www.crcv.ucf.edu/nsf-projects/reu/reu-2021/">Ethan Thomas, REU 2021</a>
		<br>
		<a href="https://www.crcv.ucf.edu/nsf-projects/reu/reu-2020/">Kali Carter, REU 2020</a>

              <br>
              <br>
		    
            </td>
          </tr>

		
					
					
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Feel free to steal this website's <a href="https://github.com/jonbarron/jonbarron_website">source code</a>. <strong>Do not</strong> scrape the HTML from this page itself, as it includes analytics tags that you do not want on your own website &mdash; use the github code instead. Also, consider using <a href="https://leonidk.com/">Leonid Keselman</a>'s <a href="https://github.com/leonidk/new_website">Jekyll fork</a> of this page.
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
